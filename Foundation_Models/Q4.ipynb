{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eab9159-42c4-4890-b1d7-25a7b1db9b0a",
   "metadata": {},
   "source": [
    "## Q4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee66fb-f3ec-465e-b36d-c29873eec4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Loading raw processed data for clinical interpretability...\n",
      "Successfully loaded preprocessed data.\n",
      "Creating raw patient features for clinical interpretation...\n",
      "Raw train features shape: (4000, 150)\n",
      "Raw test features shape: (4000, 150)\n",
      "Found 36 time series variables.\n",
      "Using 20 features for patient summaries.\n",
      "\n",
      "Sample patient summary:\n",
      "Patient Summary: Age 72 (70s), Gender Male, Height -1.0 cm, Weight 60.6 kg, HR: mean 52.2 max 60.0 min 43.0 bpm, RespRate: mean 16.6 max 20.0 min 12.0 breaths/min, SysABP: mean 125.4 max 150.0 min 94.0 mmHg, DiasABP: mean 54.6 max 67.0 min 49.0 mmHg, MAP: mean 77.4 max 96.0 min 62.0 mmHg, Temp: mean 36.6 max 37.2 min 35.1 °C, GCS: mean 13.9 max 15.0 min 10.0, PaO2: mean 123.0 max 123.0 min 123.0 mmHg, Lactate: mean 1.3 max 1.3 min 1.3 mmol/L, Creatinine: mean 0.8 max 0.9 min 0.7 mg/dL, BUN: mean 8.2 max 9.0 min 6.0 mg/dL, Glucose: mean 123.3 max 145.0 min 112.0 mg/dL, WBC: mean 8.2 max 9.0 min 7.3 K/uL, Platelets: mean 313.5 max 349.0 min 281.0 K/uL, HCT: mean 32.7 max 34.3 min 31.9 %, Urine: mean 155.0 max 400.0 min 30.0 mL, pH: mean 7.4 max 7.4 min 7.4, High-risk factors: hypothermia.\n",
      "\n",
      "--- Evaluating LLM Prompting (gemma2:2b) ---\n",
      "Testing on first 20 patients...\n",
      "testing patients with ids ['156779', '157890', '162856', '159031', '157325', '154832', '155286', '162427', '162178', '158932', '159820', '157248', '156766', '158165', '157500', '153863', '160094', '161696', '154467', '154157']\n",
      "Processing test patients...\n",
      "\n",
      "Example 0:\n",
      "Summary: Patient Summary: Age 72 (70s), Gender Male, Height -1.0 cm, Weight 60.6 kg, HR: mean 52.2 max 60.0 min 43.0 bpm, RespRate: mean 16.6 max 20.0 min 12.0 breaths/min, SysABP: mean 125.4 max 150.0 min 94.0 mmHg, DiasABP: mean 54.6 max 67.0 min 49.0 mmHg, MAP: mean 77.4 max 96.0 min 62.0 mmHg, Temp: mean 36.6 max 37.2 min 35.1 °C, GCS: mean 13.9 max 15.0 min 10.0, PaO2: mean 123.0 max 123.0 min 123.0 mmHg, Lactate: mean 1.3 max 1.3 min 1.3 mmol/L, Creatinine: mean 0.8 max 0.9 min 0.7 mg/dL, BUN: mean 8.2 max 9.0 min 6.0 mg/dL, Glucose: mean 123.3 max 145.0 min 112.0 mg/dL, WBC: mean 8.2 max 9.0 min 7.3 K/uL, Platelets: mean 313.5 max 349.0 min 281.0 K/uL, HCT: mean 32.7 max 34.3 min 31.9 %, Urine: mean 155.0 max 400.0 min 30.0 mL, pH: mean 7.4 max 7.4 min 7.4, High-risk factors: hypothermia.\n",
      "LLM Score: nan/10\n",
      "\n",
      "Example 5:\n",
      "Summary: Patient Summary: Age 52 (50s), Gender Male, Height -1.0 cm, Weight 83.0 kg, HR: mean 100.6 max 127.0 min 76.0 bpm, RespRate: mean 13.7 max 26.0 min 8.0 breaths/min, Temp: mean 37.0 max 37.3 min 36.5 °C, GCS: mean 15.0 max 15.0 min 15.0, Creatinine: mean 1.1 max 1.3 min 1.0 mg/dL, BUN: mean 16.5 max 19.0 min 14.0 mg/dL, Glucose: mean 339.3 max 806.0 min 101.0 mg/dL, WBC: mean 15.4 max 18.0 min 12.9 K/uL, Platelets: mean 406.5 max 412.0 min 401.0 K/uL, HCT: mean 24.7 max 25.6 min 23.0 %, Urine: mean 100.9 max 600.0 min 23.0 mL.\n",
      "LLM Score: nan/10\n",
      "Processed 10/20 patients\n",
      "\n",
      "Example 10:\n",
      "Summary: Patient Summary: Age 74 (70s), Gender Female, Height -1.0 cm, Weight 90.0 kg, HR: mean 103.8 max 125.0 min 87.0 bpm, SysABP: mean 110.6 max 135.0 min 83.0 mmHg, DiasABP: mean 56.7 max 87.0 min 46.0 mmHg, MAP: mean 73.0 max 103.0 min 58.0 mmHg, Temp: mean 36.9 max 37.8 min 36.2 °C, GCS: mean 11.6 max 15.0 min 3.0, PaO2: mean 117.1 max 152.0 min 79.0 mmHg, FiO2: mean 0.5 max 0.5 min 0.5, MechVent: mean 1.0 max 1.0 min 1.0, Lactate: mean 3.8 max 4.1 min 3.5 mmol/L, Creatinine: mean 0.8 max 0.8 min 0.8 mg/dL, BUN: mean 21.5 max 22.0 min 21.0 mg/dL, Glucose: mean 154.5 max 162.0 min 147.0 mg/dL, WBC: mean 14.1 max 14.4 min 13.8 K/uL, Platelets: mean 133.0 max 140.0 min 126.0 K/uL, HCT: mean 30.7 max 36.5 min 27.6 %, Urine: mean 109.0 max 400.0 min 0.0 mL, pH: mean 7.3 max 7.4 min 7.2, High-risk factors: altered mental status, hypotension.\n",
      "LLM Score: nan/10\n",
      "\n",
      "Example 15:\n",
      "Summary: Patient Summary: Age 47 (40s), Gender Female, Height -1.0 cm, Weight 63.7 kg, HR: mean 80.2 max 100.0 min 71.0 bpm, RespRate: mean 20.8 max 27.0 min 14.0 breaths/min, Temp: mean 36.3 max 36.5 min 36.1 °C, GCS: mean 14.9 max 15.0 min 14.0, PaO2: mean 67.0 max 80.0 min 54.0 mmHg, SaO2: mean 96.0 max 96.0 min 96.0, Lactate: mean 2.7 max 2.7 min 2.7 mmol/L, Creatinine: mean 0.7 max 0.9 min 0.5 mg/dL, BUN: mean 50.0 max 64.0 min 33.0 mg/dL, Glucose: mean 143.7 max 175.0 min 89.0 mg/dL, WBC: mean 8.2 max 11.3 min 4.8 K/uL, Platelets: mean 227.5 max 251.0 min 212.0 K/uL, HCT: mean 34.1 max 38.5 min 31.5 %, Urine: mean 305.6 max 1500.0 min 45.0 mL, pH: mean 7.5 max 7.5 min 7.5.\n",
      "LLM Score: nan/10\n",
      "Processed 20/20 patients\n",
      "Warning: Only 0/20 test predictions were valid.\n",
      "Check Ollama server connection and model availability.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import ollama\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "# Paths and constants\n",
    "ML_READY_PATH = 'ml_ready_data'\n",
    "PROCESSED_PATH = 'processed_data'\n",
    "MODELS_DIR = 'models'\n",
    "RESULTS_DIR = 'results'\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "STATIC_VARS = ['Age', 'Gender', 'Height', 'Weight']\n",
    "TARGET_VAR = 'In_hospital_death'\n",
    "LLM_MODEL = 'gemma2:2b'\n",
    "\n",
    "# Try to load processed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "try:\n",
    "    # Load the raw processed data for clinical interpretability\n",
    "    print(\"Loading raw processed data for clinical interpretability...\")\n",
    "    train_data_raw = pd.read_parquet(f'{PROCESSED_PATH}/set-a.parquet')\n",
    "    test_data_raw = pd.read_parquet(f'{PROCESSED_PATH}/set-c.parquet')\n",
    "    \n",
    "    # Load the patient features (aggregated data) for the target variable and indices\n",
    "    X_train_agg = pd.read_parquet(f'{ML_READY_PATH}/patient_features-a.parquet')\n",
    "    X_test_agg = pd.read_parquet(f'{ML_READY_PATH}/patient_features-c.parquet')\n",
    "    \n",
    "    # Set index to PatientID for convenience\n",
    "    X_train_agg = X_train_agg.set_index('PatientID') if 'PatientID' in X_train_agg.columns else X_train_agg\n",
    "    X_test_agg = X_test_agg.set_index('PatientID') if 'PatientID' in X_test_agg.columns else X_test_agg\n",
    "    \n",
    "    # Extract labels\n",
    "    y_train = X_train_agg[TARGET_VAR] if TARGET_VAR in X_train_agg.columns else None\n",
    "    y_test = X_test_agg[TARGET_VAR] if TARGET_VAR in X_test_agg.columns else None\n",
    "\n",
    "    print(\"Successfully loaded preprocessed data.\")\n",
    "    DATA_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"Error loading preprocessed data: {e}\")\n",
    "    DATA_AVAILABLE = False\n",
    "\n",
    "# Define helper functions for metrics calculation\n",
    "def calculate_metrics(y_true, y_pred_proba, set_name=\"Test\"):\n",
    "    \"\"\"Calculates and prints AuROC and AuPRC.\"\"\"\n",
    "    try:\n",
    "        y_true, y_pred_proba = np.asarray(y_true), np.asarray(y_pred_proba)\n",
    "        if not np.all(np.isfinite(y_true)): \n",
    "            print(f\"Warning: Non-finite y_true for {set_name}. Skip.\"); \n",
    "            return np.nan, np.nan\n",
    "        if not np.all(np.isfinite(y_pred_proba)): \n",
    "            print(f\"Warning: Non-finite y_pred_proba for {set_name}. Replace.\"); \n",
    "            y_pred_proba = np.nan_to_num(y_pred_proba, nan=0.5, posinf=1.0, neginf=0.0)\n",
    "        if len(np.unique(y_true)) < 2: \n",
    "            print(f\"Warning: Only one class in y_true for {set_name}.\"); \n",
    "            return np.nan, np.nan\n",
    "        \n",
    "        auroc = roc_auc_score(y_true, y_pred_proba)\n",
    "        auprc = average_precision_score(y_true, y_pred_proba)\n",
    "        \n",
    "        print(f\"{set_name} AuROC: {auroc:.4f}\")\n",
    "        print(f\"{set_name} AuPRC: {auprc:.4f}\")\n",
    "        return auroc, auprc\n",
    "    except ValueError as e: \n",
    "        print(f\"Metrics Error for {set_name}: {e}\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def create_raw_patient_features(data):\n",
    "    \"\"\"\n",
    "    Create patient-level features from the raw time series data.\n",
    "    \"\"\"\n",
    "    # Group by patient\n",
    "    patient_features = []\n",
    "    \n",
    "    for patient_id in data['PatientID'].unique():\n",
    "        # Get the patient's data\n",
    "        patient_data = data[data['PatientID'] == patient_id].copy()\n",
    "        \n",
    "        # Sort by hour\n",
    "        patient_data = patient_data.sort_values('Hour')\n",
    "        \n",
    "        # Initialize feature dict with patient ID and target\n",
    "        features = {\n",
    "            'PatientID': patient_id,\n",
    "            TARGET_VAR: patient_data[TARGET_VAR].iloc[0]  # All rows have the same target value\n",
    "        }\n",
    "        \n",
    "        # Add static features (use the first row since they're constant)\n",
    "        for var in STATIC_VARS:\n",
    "            if var in patient_data.columns:\n",
    "                features[var] = patient_data[var].iloc[0]\n",
    "        \n",
    "        # Add ICUType as a reference (not for training)\n",
    "        if 'ICUType' in patient_data.columns:\n",
    "            features['ICUType'] = patient_data['ICUType'].iloc[0]\n",
    "        \n",
    "        # Identify time series variables\n",
    "        id_vars = ['PatientID', 'RecordID', 'Hour']\n",
    "        exclude_cols = STATIC_VARS + id_vars + ['ICUType', TARGET_VAR]\n",
    "        time_series_vars = [col for col in patient_data.columns if col not in exclude_cols]\n",
    "        \n",
    "        # For time series variables, calculate summary statistics\n",
    "        for var in time_series_vars:\n",
    "            # Skip if all values are NaN\n",
    "            if patient_data[var].isna().all():\n",
    "                continue\n",
    "                \n",
    "            # Basic statistics\n",
    "            features[f'{var}_mean'] = patient_data[var].mean()\n",
    "            features[f'{var}_std'] = patient_data[var].std()\n",
    "            features[f'{var}_min'] = patient_data[var].min()\n",
    "            features[f'{var}_max'] = patient_data[var].max()\n",
    "        \n",
    "        patient_features.append(features)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    patient_features_df = pd.DataFrame(patient_features)\n",
    "    \n",
    "    return patient_features_df\n",
    "\n",
    "# If we haven't created raw patient features yet, do it now\n",
    "print(\"Creating raw patient features for clinical interpretation...\")\n",
    "raw_train_features = create_raw_patient_features(train_data_raw)\n",
    "raw_test_features = create_raw_patient_features(test_data_raw)\n",
    "\n",
    "# Set index to PatientID\n",
    "raw_train_features = raw_train_features.set_index('PatientID')\n",
    "raw_test_features = raw_test_features.set_index('PatientID')\n",
    "\n",
    "# Print some info about the raw features\n",
    "print(f\"Raw train features shape: {raw_train_features.shape}\")\n",
    "print(f\"Raw test features shape: {raw_test_features.shape}\")\n",
    "\n",
    "# Identify time series variables from the raw data\n",
    "id_vars = ['PatientID', 'RecordID', 'Hour']\n",
    "exclude_cols = STATIC_VARS + id_vars + ['ICUType', TARGET_VAR]\n",
    "TIME_SERIES_VARS = [col for col in train_data_raw.columns if col not in exclude_cols]\n",
    "print(f\"Found {len(TIME_SERIES_VARS)} time series variables.\")\n",
    "\n",
    "# Function to create text summary from patient features\n",
    "def create_text_summary_from_agg(patient_id, df_agg_features, feats_to_include):\n",
    "    \"\"\"\n",
    "    Generates text summary from aggregated patient features.\n",
    "    Uses the pre-computed aggregated features from your preprocessing.\n",
    "    \"\"\"\n",
    "    if patient_id not in df_agg_features.index: \n",
    "        return \"Patient data not found.\"\n",
    "    \n",
    "    patient_data_agg = df_agg_features.loc[patient_id]\n",
    "    summary = []\n",
    "    \n",
    "    # Add static features\n",
    "    for feat in STATIC_VARS:\n",
    "        if feat in patient_data_agg:\n",
    "            value = patient_data_agg[feat]\n",
    "            if pd.isna(value):\n",
    "                continue\n",
    "                \n",
    "            # Format demographic information based on feature type\n",
    "            if feat == 'Age':\n",
    "                age = int(value)\n",
    "                age_group = \"\"\n",
    "                if age < 30: age_group = \"(young adult)\"\n",
    "                elif age < 40: age_group = \"(30s)\"\n",
    "                elif age < 50: age_group = \"(40s)\"\n",
    "                elif age < 60: age_group = \"(50s)\"\n",
    "                elif age < 70: age_group = \"(60s)\"\n",
    "                elif age < 80: age_group = \"(70s)\"\n",
    "                else: age_group = \"(elderly)\"\n",
    "                summary.append(f\"Age {age} {age_group}\")\n",
    "            \n",
    "            elif feat == 'Gender':\n",
    "                gender = \"Male\" if round(value) == 1 else \"Female\"\n",
    "                summary.append(f\"Gender {gender}\")\n",
    "            \n",
    "            elif feat == 'Height':\n",
    "                summary.append(f\"Height {value:.1f} cm\")\n",
    "            \n",
    "            elif feat == 'Weight':\n",
    "                summary.append(f\"Weight {value:.1f} kg\")\n",
    "    \n",
    "    # Calculate BMI if both height and weight are available\n",
    "    if 'Height' in patient_data_agg and 'Weight' in patient_data_agg:\n",
    "        height = patient_data_agg['Height']\n",
    "        weight = patient_data_agg['Weight']\n",
    "        if not pd.isna(height) and not pd.isna(weight) and height > 0 and weight > 0:\n",
    "            height_m = height / 100\n",
    "            bmi = weight / (height_m * height_m)\n",
    "            bmi_category = \"\"\n",
    "            if bmi < 18.5: bmi_category = \"(underweight)\"\n",
    "            elif bmi < 25: bmi_category = \"(normal)\"\n",
    "            elif bmi < 30: bmi_category = \"(overweight)\"\n",
    "            else: bmi_category = \"(obese)\"\n",
    "            summary.append(f\"BMI {bmi:.1f} {bmi_category}\")\n",
    "    \n",
    "    # Track high-risk conditions based on clinical guidelines\n",
    "    high_risk_conditions = []\n",
    "    \n",
    "    # Process dynamic features using the aggregated statistics\n",
    "    for feat in feats_to_include:\n",
    "        # Look for derived statistics columns in the aggregated features\n",
    "        mean_col = f'{feat}_mean'\n",
    "        max_col = f'{feat}_max'\n",
    "        min_col = f'{feat}_min'\n",
    "        \n",
    "        # Check if these columns exist in patient data\n",
    "        feat_summary = []\n",
    "        \n",
    "        # Add mean value if available\n",
    "        if mean_col in patient_data_agg:\n",
    "            mean_val = patient_data_agg[mean_col]\n",
    "            if not pd.isna(mean_val):\n",
    "                feat_summary.append(f\"mean {mean_val:.1f}\")\n",
    "                \n",
    "                # Check for specific risk thresholds based on clinical knowledge\n",
    "                if feat == 'HR' and mean_val > 120: \n",
    "                    high_risk_conditions.append(\"tachycardia\")\n",
    "                elif feat == 'HR' and mean_val < 50: \n",
    "                    high_risk_conditions.append(\"bradycardia\")\n",
    "                elif feat == 'RespRate' and mean_val > 30: \n",
    "                    high_risk_conditions.append(\"respiratory distress\")\n",
    "                elif feat == 'RespRate' and mean_val < 8: \n",
    "                    high_risk_conditions.append(\"respiratory depression\")\n",
    "                elif feat == 'MAP' and mean_val < 65: \n",
    "                    high_risk_conditions.append(\"low MAP\")\n",
    "                elif feat == 'GCS' and mean_val < 8: \n",
    "                    high_risk_conditions.append(\"severe altered mental status\")\n",
    "                elif feat == 'GCS' and mean_val < 13: \n",
    "                    high_risk_conditions.append(\"altered mental status\")\n",
    "                elif feat == 'Lactate' and mean_val > 4.0: \n",
    "                    high_risk_conditions.append(\"severe lactic acidosis\")\n",
    "                elif feat == 'Creatinine' and mean_val > 1.3: \n",
    "                    high_risk_conditions.append(\"renal dysfunction\")\n",
    "        \n",
    "        # Add max value if available\n",
    "        if max_col in patient_data_agg:\n",
    "            max_val = patient_data_agg[max_col]\n",
    "            if not pd.isna(max_val):\n",
    "                feat_summary.append(f\"max {max_val:.1f}\")\n",
    "                \n",
    "                # Additional checks based on max values\n",
    "                if feat == 'Temp' and max_val > 38.5:\n",
    "                    high_risk_conditions.append(\"fever\")\n",
    "                elif feat == 'SysABP' and max_val > 160:\n",
    "                    high_risk_conditions.append(\"hypertension\")\n",
    "        \n",
    "        # Add min value if available\n",
    "        if min_col in patient_data_agg:\n",
    "            min_val = patient_data_agg[min_col]\n",
    "            if not pd.isna(min_val):\n",
    "                feat_summary.append(f\"min {min_val:.1f}\")\n",
    "                \n",
    "                # Additional checks based on min values\n",
    "                if feat == 'SysABP' and min_val < 90:\n",
    "                    high_risk_conditions.append(\"hypotension\")\n",
    "                elif feat == 'Temp' and min_val < 36.0:\n",
    "                    high_risk_conditions.append(\"hypothermia\")\n",
    "        \n",
    "        # Add feature summary if we have any data\n",
    "        if feat_summary:\n",
    "            # Add appropriate units based on the variable\n",
    "            units = \"\"\n",
    "            if feat in ['HR']: units = \"bpm\"\n",
    "            elif feat in ['RespRate']: units = \"breaths/min\"\n",
    "            elif feat in ['SysABP', 'DiasABP', 'MAP', 'NISysABP', 'NIDiasABP', 'NIMAP']: units = \"mmHg\"\n",
    "            elif feat in ['Temp']: units = \"°C\"\n",
    "            elif feat in ['Glucose']: units = \"mg/dL\"\n",
    "            elif feat in ['Lactate']: units = \"mmol/L\"\n",
    "            elif feat in ['Creatinine', 'BUN']: units = \"mg/dL\"\n",
    "            elif feat in ['HCT']: units = \"%\"\n",
    "            elif feat in ['WBC', 'Platelets']: units = \"K/uL\"\n",
    "            elif feat in ['PaO2', 'PaCO2']: units = \"mmHg\"\n",
    "            elif feat in ['Urine']: units = \"mL\"\n",
    "            \n",
    "            summary.append(f\"{feat}: {' '.join(feat_summary)}{' ' + units if units else ''}\")\n",
    "    \n",
    "    # Add high risk conditions if any\n",
    "    if high_risk_conditions:\n",
    "        unique_conditions = list(set(high_risk_conditions))  # Remove duplicates\n",
    "        summary.append(f\"High-risk factors: {', '.join(unique_conditions)}\")\n",
    "    \n",
    "    return \"Patient Summary: \" + \", \".join(summary) + \".\"\n",
    "\n",
    "# Create a prompt template string that can be reused\n",
    "prompt_template = \"\"\"Given the following ICU patient summary over 48 hours:\n",
    "{summary_text}\n",
    "\n",
    "As a medical expert, estimate the likelihood of in-hospital mortality based ONLY on this data.\n",
    "Consider these risk factors (based on analysis of ICU patient data):\n",
    "\n",
    "1. Demographics:\n",
    "   - Advanced age (particularly >70 years)\n",
    "   - Gender (males have slightly higher mortality in some ICU settings)\n",
    "\n",
    "2. Vital Signs:\n",
    "   - Low or high heart rate (bradycardia <50, tachycardia >120)\n",
    "   - Abnormal blood pressure (SBP <90 or >160, MAP <65)\n",
    "   - Abnormal temperature (hypothermia <36°C, fever >38.5°C)\n",
    "   - Respiratory distress (RR >30 or <8)\n",
    "\n",
    "3. Neurological:\n",
    "   - Low GCS (Glasgow Coma Scale) - especially if <8\n",
    "   - Altered mental status\n",
    "\n",
    "4. Laboratory:\n",
    "   - Elevated lactate (>2 mmol/L, especially >4)\n",
    "   - Abnormal renal function (high creatinine >1.3, high BUN >20)\n",
    "   - Abnormal WBC (leukocytosis or leukopenia)\n",
    "   - Thrombocytopenia (platelets <150K)\n",
    "   - Severe anemia (HCT <30%)\n",
    "\n",
    "5. Respiratory:\n",
    "   - Hypoxemia (low PaO2 or SaO2)\n",
    "   - High oxygen requirements (FiO2 >0.5)\n",
    "   - Mechanical ventilation\n",
    "   \n",
    "6. Other:\n",
    "   - Low urine output\n",
    "   - Multiple organ dysfunction\n",
    "\n",
    "Rate the mortality risk on a scale of 1-10 where:\n",
    "1 = Very low risk (<5% chance of death)\n",
    "10 = Very high risk (>80% chance of death)\n",
    "\n",
    "Respond with ONLY the integer number between 1 and 10.\n",
    "Score:\"\"\"\n",
    "\n",
    "# Function to query the LLM for a mortality score (with batching optimization)\n",
    "def query_llm_score_with_retries(summary_text, model_name, max_retries=3):\n",
    "    \"\"\"Query LLM with retry logic\"\"\"\n",
    "    prompt = prompt_template.format(summary_text=summary_text)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try: \n",
    "            response = ollama.chat(model=model_name, messages=[{'role': 'user', 'content': prompt}])\n",
    "            content = response['message']['content']\n",
    "            \n",
    "            # Try to extract just the score number\n",
    "            match = re.search(r'\\b([1-9]|10)\\b', content)\n",
    "            score = int(match.group(0)) if match else np.nan\n",
    "            \n",
    "            # Fallback extraction if primary fails\n",
    "            if pd.isna(score): \n",
    "                match_digit = re.search(r'\\d+', content)\n",
    "                score = max(1, min(10, int(match_digit.group(0)))) if match_digit else np.nan\n",
    "            \n",
    "            if not pd.isna(score):\n",
    "                return score\n",
    "            \n",
    "            # If we're here, we couldn't parse a valid score\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)  # Short delay before retry\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)  # Short delay before retry\n",
    "    \n",
    "    return np.nan  # Return NaN if all attempts failed\n",
    "\n",
    "# Optimized batch processing function with a thread pool\n",
    "def process_patients_in_batches(patient_ids, features_df, features_for_summary):\n",
    "    \"\"\"Process patients in batches using thread pool for parallel processing\"\"\"\n",
    "    # Create a thread pool\n",
    "    scores = []\n",
    "    batch_size = 10  # Adjust based on your system capabilities\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        # Process patients in batches\n",
    "        for batch_start in range(0, len(patient_ids), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(patient_ids))\n",
    "            batch_ids = patient_ids[batch_start:batch_end]\n",
    "            \n",
    "            # Create patient summaries for this batch\n",
    "            summaries = [create_text_summary_from_agg(pid, features_df, features_for_summary) for pid in batch_ids]\n",
    "            \n",
    "            # Submit queries to the thread pool\n",
    "            future_to_idx = {executor.submit(query_llm_score_with_retries, summary, LLM_MODEL): i \n",
    "                            for i, summary in enumerate(summaries)}\n",
    "            \n",
    "            # Process completed futures\n",
    "            batch_scores = [np.nan] * len(summaries)\n",
    "            for future in concurrent.futures.as_completed(future_to_idx):\n",
    "                idx = future_to_idx[future]\n",
    "                try:\n",
    "                    score = future.result()\n",
    "                    batch_scores[idx] = score\n",
    "                    \n",
    "                    # Print occasional updates (every 5th patient)\n",
    "                    if (batch_start + idx) % 5 == 0:\n",
    "                        summary = summaries[idx]\n",
    "                        print(f\"\\nExample {batch_start + idx}:\")\n",
    "                        print(f\"Summary: {summary}\")\n",
    "                        print(f\"LLM Score: {score}/10\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing patient: {e}\")\n",
    "            \n",
    "            # Add batch scores to overall scores\n",
    "            scores.extend(batch_scores)\n",
    "            \n",
    "            # Show progress\n",
    "            print(f\"Processed {batch_end}/{len(patient_ids)} patients\")\n",
    "            \n",
    "    return scores\n",
    "\n",
    "# Define features for the patient summaries\n",
    "features_for_summary_llm = [\n",
    "    # Vital signs (well-measured with clinical relevance)\n",
    "    'HR', 'RespRate', 'SysABP', 'DiasABP', 'MAP', 'Temp',\n",
    "    \n",
    "    # Neurological status (strong predictor of outcomes)\n",
    "    'GCS',\n",
    "    \n",
    "    # Respiratory parameters\n",
    "    'PaO2', 'SaO2', 'FiO2', 'MechVent',\n",
    "    \n",
    "    # Key lab values\n",
    "    'Lactate', 'Creatinine', 'BUN', 'Glucose', 'WBC', 'Platelets', 'HCT',\n",
    "    \n",
    "    # Other important indicators\n",
    "    'Urine', 'pH'\n",
    "]\n",
    "\n",
    "# Verify features exist in the raw data\n",
    "available_features = []\n",
    "for feature in features_for_summary_llm:\n",
    "    mean_feature = f\"{feature}_mean\"\n",
    "    if mean_feature in raw_train_features.columns:\n",
    "        available_features.append(feature)\n",
    "\n",
    "missing_features = set(features_for_summary_llm) - set(available_features)\n",
    "if missing_features:\n",
    "    print(f\"Warning: Some desired features not found: {missing_features}\")\n",
    "    features_for_summary_llm = available_features\n",
    "\n",
    "print(f\"Using {len(features_for_summary_llm)} features for patient summaries.\")\n",
    "\n",
    "# Test one patient summary to verify it looks reasonable\n",
    "test_pid = X_test_agg.index[0]\n",
    "test_summary = create_text_summary_from_agg(test_pid, raw_test_features, features_for_summary_llm)\n",
    "print(f\"\\nSample patient summary:\\n{test_summary}\")\n",
    "\n",
    "# Main execution\n",
    "print(f\"\\n--- Evaluating LLM Prompting ({LLM_MODEL}) ---\")\n",
    "\n",
    "# Process a subset of test patients first\n",
    "test_size = 20  # Modify this for your needs (start small!)\n",
    "print(f\"Testing on first {test_size} patients...\")\n",
    "test_patient_ids = X_test_agg.index[:test_size].tolist()\n",
    "print(\"testing patients with ids \" + str(test_patient_ids))\n",
    "\n",
    "print(\"Processing test patients...\")\n",
    "test_scores = process_patients_in_batches(test_patient_ids, raw_test_features, features_for_summary_llm)\n",
    "\n",
    "# Validate scores and convert to probabilities (1-10 scale to 0-1)\n",
    "valid_scores = [s for s in test_scores if not pd.isna(s)]\n",
    "if len(valid_scores) < len(test_scores) * 0.8:  # Less than 80% valid scores\n",
    "    print(f\"Warning: Only {len(valid_scores)}/{len(test_scores)} test predictions were valid.\")\n",
    "    print(\"Check Ollama server connection and model availability.\")\n",
    "    proceed = input(\"Do you want to continue with more patients? (y/n): \")\n",
    "    if proceed.lower() != 'y':\n",
    "        print(\"Stopping after test run\")\n",
    "    else:\n",
    "        # Process remaining patients if user wants to continue\n",
    "        remaining_size = 100  # Adjust as needed\n",
    "        if remaining_size > 0:\n",
    "            print(f\"Processing additional {remaining_size} patients...\")\n",
    "            additional_patient_ids = X_test_agg.index[test_size:test_size+remaining_size].tolist()\n",
    "            additional_scores = process_patients_in_batches(additional_patient_ids, raw_test_features, features_for_summary_llm)\n",
    "            test_patient_ids.extend(additional_patient_ids)\n",
    "            test_scores.extend(additional_scores)\n",
    "else:\n",
    "    # If test set was successful, continue with more patients\n",
    "    remaining_size = 100  # Adjust as needed\n",
    "    if remaining_size > 0:\n",
    "        print(f\"Test run successful. Processing additional {remaining_size} patients...\")\n",
    "        additional_patient_ids = X_test_agg.index[test_size:test_size+remaining_size].tolist()\n",
    "        additional_scores = process_patients_in_batches(additional_patient_ids, raw_test_features, features_for_summary_llm)\n",
    "        test_patient_ids.extend(additional_patient_ids)\n",
    "        test_scores.extend(additional_scores)\n",
    "\n",
    "# Convert scores to probabilities and evaluate performance\n",
    "y_true = y_test.loc[test_patient_ids].values\n",
    "y_pred_proba = (np.array(test_scores, dtype=float) - 1.0) / 9.0  # Convert 1-10 to 0-1 scale\n",
    "\n",
    "# Handle missing predictions\n",
    "valid_indices = ~np.isnan(y_pred_proba) & ~np.isnan(y_true)\n",
    "\n",
    "if np.sum(valid_indices) == 0:\n",
    "    print(\"Error: No valid LLM scores/targets.\")\n",
    "else:\n",
    "    print(f\"Valid LLM scores: {np.sum(valid_indices)}/{len(y_true)}\")\n",
    "    \n",
    "    # Calculate and report metrics\n",
    "    auroc_llm, auprc_llm = calculate_metrics(\n",
    "        y_true[valid_indices], \n",
    "        y_pred_proba[valid_indices], \n",
    "        set_name=f\"Test (LLM Prompt {LLM_MODEL})\"\n",
    "    )\n",
    "    \n",
    "    # Save results \n",
    "    with open(f\"{RESULTS_DIR}/llm_prompt_results.txt\", \"w\") as f:\n",
    "        f.write(f\"LLM Model: {LLM_MODEL}\\n\")\n",
    "        f.write(f\"Valid predictions: {np.sum(valid_indices)}/{len(y_true)}\\n\")\n",
    "        f.write(f\"AuROC: {auroc_llm:.4f}\\n\")\n",
    "        f.write(f\"AuPRC: {auprc_llm:.4f}\\n\")\n",
    "        \n",
    "    # Create a histogram of prediction scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(np.array(test_scores)[valid_indices], bins=10, alpha=0.7)\n",
    "    plt.title(f'Distribution of LLM Risk Scores ({LLM_MODEL})')\n",
    "    plt.xlabel('Risk Score (1-10)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(f\"{RESULTS_DIR}/llm_score_distribution.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(f\"\\n--- Finished LLM Evaluation ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b26bd67-0df6-48b3-a81a-b8bee291c90b",
   "metadata": {},
   "source": [
    "## Q4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125eda4-d378-42d2-8634-95e3043627dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "import warnings\n",
    "import ollama\n",
    "import concurrent.futures\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths (matching your preprocessing code)\n",
    "ML_READY_PATH = 'ml_ready_data'   # Where your scaled and feature-engineered files are\n",
    "PROCESSED_PATH = 'processed_data' # Where your initial imputed files are\n",
    "MODELS_DIR = 'models'             # Directory to save trained models\n",
    "RESULTS_DIR = 'results'           # Directory to save plots and result summaries\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "STATIC_VARS = ['Age', 'Gender', 'Height', 'Weight']\n",
    "TARGET_VAR = 'In_hospital_death'\n",
    "LLM_MODEL = 'gemma2:2b'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Try to load processed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "try:\n",
    "    # Load the patient features (aggregated data)\n",
    "    X_train_agg = pd.read_parquet(f'{ML_READY_PATH}/patient_features-a.parquet')\n",
    "    X_test_agg = pd.read_parquet(f'{ML_READY_PATH}/patient_features-c.parquet')\n",
    "    \n",
    "    # Set index to PatientID for convenience\n",
    "    X_train_agg = X_train_agg.set_index('PatientID') if 'PatientID' in X_train_agg.columns else X_train_agg\n",
    "    X_test_agg = X_test_agg.set_index('PatientID') if 'PatientID' in X_test_agg.columns else X_test_agg\n",
    "    \n",
    "    # Extract labels\n",
    "    y_train = X_train_agg[TARGET_VAR] if TARGET_VAR in X_train_agg.columns else None\n",
    "    y_test = X_test_agg[TARGET_VAR] if TARGET_VAR in X_test_agg.columns else None\n",
    "    \n",
    "    # Identify time series variables by looking at column names\n",
    "    TIME_SERIES_VARS = []\n",
    "    for col in X_train_agg.columns:\n",
    "        if '_mean' in col:\n",
    "            base_var = col.replace('_mean', '')\n",
    "            if base_var not in STATIC_VARS and base_var != TARGET_VAR:\n",
    "                TIME_SERIES_VARS.append(base_var)\n",
    "    \n",
    "    TIME_SERIES_VARS = list(set(TIME_SERIES_VARS))  # Deduplicate\n",
    "    print(f\"Found {len(TIME_SERIES_VARS)} time series variables.\")\n",
    "    \n",
    "    print(\"Successfully loaded preprocessed data.\")\n",
    "    DATA_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"Error loading preprocessed data: {e}\")\n",
    "    DATA_AVAILABLE = False\n",
    "\n",
    "# Define helper functions for metrics calculation\n",
    "def calculate_metrics(y_true, y_pred_proba, set_name=\"Test\"):\n",
    "    \"\"\"Calculates and prints AuROC and AuPRC.\"\"\"\n",
    "    try:\n",
    "        y_true, y_pred_proba = np.asarray(y_true), np.asarray(y_pred_proba)\n",
    "        if not np.all(np.isfinite(y_true)): \n",
    "            print(f\"Warning: Non-finite y_true for {set_name}. Skip.\")\n",
    "            return np.nan, np.nan\n",
    "        if not np.all(np.isfinite(y_pred_proba)): \n",
    "            print(f\"Warning: Non-finite y_pred_proba for {set_name}. Replace.\")\n",
    "            y_pred_proba = np.nan_to_num(y_pred_proba, nan=0.5, posinf=1.0, neginf=0.0)\n",
    "        if len(np.unique(y_true)) < 2: \n",
    "            print(f\"Warning: Only one class in y_true for {set_name}.\")\n",
    "            return np.nan, np.nan\n",
    "            \n",
    "        auroc = roc_auc_score(y_true, y_pred_proba)\n",
    "        auprc = average_precision_score(y_true, y_pred_proba)\n",
    "        \n",
    "        print(f\"{set_name} AuROC: {auroc:.4f}\")\n",
    "        print(f\"{set_name} AuPRC: {auprc:.4f}\")\n",
    "        return auroc, auprc\n",
    "    except ValueError as e: \n",
    "        print(f\"Metrics Error for {set_name}: {e}\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "# Function for creating text summary from patient features\n",
    "def create_text_summary_from_agg(patient_id, df_agg_features, feats_to_include):\n",
    "    \"\"\"\n",
    "    Generates text summary from aggregated patient features.\n",
    "    Uses the pre-computed aggregated features from your preprocessing.\n",
    "    \"\"\"\n",
    "    if patient_id not in df_agg_features.index: \n",
    "        return \"Patient data not found.\"\n",
    "    \n",
    "    patient_data_agg = df_agg_features.loc[patient_id]\n",
    "    summary = []\n",
    "    \n",
    "    # Add static features\n",
    "    for feat in STATIC_VARS:\n",
    "        if feat in patient_data_agg:\n",
    "            value = patient_data_agg[feat]\n",
    "            if pd.isna(value):\n",
    "                continue\n",
    "                \n",
    "            # Format demographic information based on feature type\n",
    "            if feat == 'Age':\n",
    "                age = int(value)\n",
    "                age_group = \"\"\n",
    "                if age < 30: age_group = \"(young adult)\"\n",
    "                elif age < 40: age_group = \"(30s)\"\n",
    "                elif age < 50: age_group = \"(40s)\"\n",
    "                elif age < 60: age_group = \"(50s)\"\n",
    "                elif age < 70: age_group = \"(60s)\"\n",
    "                elif age < 80: age_group = \"(70s)\"\n",
    "                else: age_group = \"(elderly)\"\n",
    "                summary.append(f\"Age {age} {age_group}\")\n",
    "            \n",
    "            elif feat == 'Gender':\n",
    "                gender = \"Male\" if round(value) == 1 else \"Female\"\n",
    "                summary.append(f\"Gender {gender}\")\n",
    "            \n",
    "            elif feat == 'Height':\n",
    "                summary.append(f\"Height {value:.1f} cm\")\n",
    "            \n",
    "            elif feat == 'Weight':\n",
    "                summary.append(f\"Weight {value:.1f} kg\")\n",
    "    \n",
    "    # Calculate BMI if both height and weight are available\n",
    "    if 'Height' in patient_data_agg and 'Weight' in patient_data_agg:\n",
    "        height = patient_data_agg['Height']\n",
    "        weight = patient_data_agg['Weight']\n",
    "        if not pd.isna(height) and not pd.isna(weight) and height > 0 and weight > 0:\n",
    "            height_m = height / 100\n",
    "            bmi = weight / (height_m * height_m)\n",
    "            bmi_category = \"\"\n",
    "            if bmi < 18.5: bmi_category = \"(underweight)\"\n",
    "            elif bmi < 25: bmi_category = \"(normal)\"\n",
    "            elif bmi < 30: bmi_category = \"(overweight)\"\n",
    "            else: bmi_category = \"(obese)\"\n",
    "            summary.append(f\"BMI {bmi:.1f} {bmi_category}\")\n",
    "    \n",
    "    # Track high-risk conditions based on clinical guidelines\n",
    "    high_risk_conditions = []\n",
    "    \n",
    "    # Process dynamic features using the aggregated statistics\n",
    "    for feat in feats_to_include:\n",
    "        # Look for derived statistics columns in the aggregated features\n",
    "        mean_col = f'{feat}_mean'\n",
    "        max_col = f'{feat}_max'\n",
    "        min_col = f'{feat}_min'\n",
    "        \n",
    "        # Check if these columns exist in patient data\n",
    "        feat_summary = []\n",
    "        \n",
    "        # Add mean value if available\n",
    "        if mean_col in patient_data_agg:\n",
    "            mean_val = patient_data_agg[mean_col]\n",
    "            if not pd.isna(mean_val):\n",
    "                feat_summary.append(f\"mean {mean_val:.1f}\")\n",
    "                \n",
    "                # Check for specific risk thresholds based on clinical knowledge\n",
    "                if feat == 'HR' and mean_val > 120: \n",
    "                    high_risk_conditions.append(\"tachycardia\")\n",
    "                elif feat == 'HR' and mean_val < 50: \n",
    "                    high_risk_conditions.append(\"bradycardia\")\n",
    "                elif feat == 'RespRate' and mean_val > 30: \n",
    "                    high_risk_conditions.append(\"respiratory distress\")\n",
    "                elif feat == 'RespRate' and mean_val < 8: \n",
    "                    high_risk_conditions.append(\"respiratory depression\")\n",
    "                elif feat == 'MAP' and mean_val < 65: \n",
    "                    high_risk_conditions.append(\"low MAP\")\n",
    "                elif feat == 'GCS' and mean_val < 8: \n",
    "                    high_risk_conditions.append(\"severe altered mental status\")\n",
    "                elif feat == 'GCS' and mean_val < 13: \n",
    "                    high_risk_conditions.append(\"altered mental status\")\n",
    "                elif feat == 'Lactate' and mean_val > 4.0: \n",
    "                    high_risk_conditions.append(\"severe lactic acidosis\")\n",
    "                elif feat == 'Creatinine' and mean_val > 1.3: \n",
    "                    high_risk_conditions.append(\"renal dysfunction\")\n",
    "        \n",
    "        # Add max value if available\n",
    "        if max_col in patient_data_agg:\n",
    "            max_val = patient_data_agg[max_col]\n",
    "            if not pd.isna(max_val):\n",
    "                feat_summary.append(f\"max {max_val:.1f}\")\n",
    "                \n",
    "                # Additional checks based on max values\n",
    "                if feat == 'Temp' and max_val > 38.5:\n",
    "                    high_risk_conditions.append(\"fever\")\n",
    "                elif feat == 'SysABP' and max_val > 160:\n",
    "                    high_risk_conditions.append(\"hypertension\")\n",
    "        \n",
    "        # Add min value if available\n",
    "        if min_col in patient_data_agg:\n",
    "            min_val = patient_data_agg[min_col]\n",
    "            if not pd.isna(min_val):\n",
    "                feat_summary.append(f\"min {min_val:.1f}\")\n",
    "                \n",
    "                # Additional checks based on min values\n",
    "                if feat == 'SysABP' and min_val < 90:\n",
    "                    high_risk_conditions.append(\"hypotension\")\n",
    "                elif feat == 'Temp' and min_val < 36.0:\n",
    "                    high_risk_conditions.append(\"hypothermia\")\n",
    "        \n",
    "        # Add feature summary if we have any data\n",
    "        if feat_summary:\n",
    "            # Add appropriate units based on the variable\n",
    "            units = \"\"\n",
    "            if feat in ['HR']: units = \"bpm\"\n",
    "            elif feat in ['RespRate']: units = \"breaths/min\"\n",
    "            elif feat in ['SysABP', 'DiasABP', 'MAP', 'NISysABP', 'NIDiasABP', 'NIMAP']: units = \"mmHg\"\n",
    "            elif feat in ['Temp']: units = \"°C\"\n",
    "            elif feat in ['Glucose']: units = \"mg/dL\"\n",
    "            elif feat in ['Lactate']: units = \"mmol/L\"\n",
    "            elif feat in ['Creatinine', 'BUN']: units = \"mg/dL\"\n",
    "            elif feat in ['HCT']: units = \"%\"\n",
    "            elif feat in ['WBC', 'Platelets']: units = \"K/uL\"\n",
    "            elif feat in ['PaO2', 'PaCO2']: units = \"mmHg\"\n",
    "            elif feat in ['Urine']: units = \"mL\"\n",
    "            \n",
    "            summary.append(f\"{feat}: {' '.join(feat_summary)}{' ' + units if units else ''}\")\n",
    "    \n",
    "    # Add high risk conditions if any\n",
    "    if high_risk_conditions:\n",
    "        unique_conditions = list(set(high_risk_conditions))  # Remove duplicates\n",
    "        summary.append(f\"High-risk factors: {', '.join(unique_conditions)}\")\n",
    "    \n",
    "    return \"Patient Summary: \" + \", \".join(summary) + \".\"\n",
    "\n",
    "# Function to get embeddings from Ollama with retry logic\n",
    "def get_ollama_embedding(text, model_name, max_retries=3):\n",
    "    \"\"\"Get vector embedding for text from Ollama model with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try: \n",
    "            response = ollama.embeddings(model=model_name, prompt=text)\n",
    "            return response['embedding']\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)  # Short delay before retry\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Embedding Error ({model_name}): {e}\")\n",
    "                return None\n",
    "\n",
    "# Process patient summaries in parallel\n",
    "def process_summaries_parallel(patients, features_df, features_list, max_workers=5):\n",
    "    \"\"\"Generate patient summaries in parallel\"\"\"\n",
    "    summaries = {}\n",
    "    \n",
    "    def process_patient(pid):\n",
    "        return pid, create_text_summary_from_agg(pid, features_df, features_list)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_patient, pid) for pid in patients]\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Generating summaries\"):\n",
    "            pid, summary = future.result()\n",
    "            summaries[pid] = summary\n",
    "            \n",
    "    return summaries\n",
    "\n",
    "# Get embeddings in parallel with batching\n",
    "def get_embeddings_parallel(summaries, model_name, batch_size=20, max_workers=5):\n",
    "    \"\"\"Get embeddings in parallel with batching\"\"\"\n",
    "    all_embeddings = {}\n",
    "    patient_ids = list(summaries.keys())\n",
    "    total_batches = (len(patient_ids) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx in range(total_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(patient_ids))\n",
    "        batch_ids = patient_ids[start_idx:end_idx]\n",
    "        print(f\"Processing batch {batch_idx+1}/{total_batches} ({len(batch_ids)} patients)\")\n",
    "        \n",
    "        # Process embeddings in parallel\n",
    "        def get_embedding_for_patient(pid):\n",
    "            return pid, get_ollama_embedding(summaries[pid], model_name)\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(get_embedding_for_patient, pid) for pid in batch_ids]\n",
    "            \n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Generating embeddings\"):\n",
    "                try:\n",
    "                    pid, embedding = future.result()\n",
    "                    if embedding is not None:\n",
    "                        all_embeddings[pid] = embedding\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in embedding generation: {e}\")\n",
    "        \n",
    "        # Add a short pause between batches to avoid overloading the Ollama server\n",
    "        if batch_idx < total_batches - 1:\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# --- Main execution for Q4.2: Using LLMs to retrieve embeddings ---\n",
    "def run_llm_embeddings():\n",
    "    print(f\"\\n--- Q4.2: Using LLMs to retrieve embeddings ({LLM_MODEL}) ---\")\n",
    "    \n",
    "    # Get patient IDs from both train and test sets\n",
    "    train_patient_ids = X_train_agg.index.tolist()\n",
    "    test_patient_ids = X_test_agg.index.tolist()\n",
    "    \n",
    "    # Select features based on clinical importance\n",
    "    features_for_summary_llm = [\n",
    "        # Vital signs\n",
    "        'HR', 'RespRate', 'SysABP', 'DiasABP', 'MAP', 'Temp',\n",
    "        # Neurological\n",
    "        'GCS',\n",
    "        # Respiratory\n",
    "        'PaO2', 'SaO2', 'FiO2', 'MechVent',\n",
    "        # Key lab values\n",
    "        'Lactate', 'Creatinine', 'BUN', 'Glucose', 'WBC', 'Platelets', 'HCT',\n",
    "        # Other\n",
    "        'Urine', 'pH'\n",
    "    ]\n",
    "\n",
    "    # Verify features exist in the data\n",
    "    available_features = []\n",
    "    for feature in features_for_summary_llm:\n",
    "        mean_feature = f\"{feature}_mean\"\n",
    "        if mean_feature in X_train_agg.columns:\n",
    "            available_features.append(feature)\n",
    "    \n",
    "    missing_features = set(features_for_summary_llm) - set(available_features)\n",
    "    if missing_features:\n",
    "        print(f\"Warning: Some desired features not found: {missing_features}\")\n",
    "        features_for_summary_llm = available_features\n",
    "        \n",
    "    print(f\"Using {len(features_for_summary_llm)} features for patient summaries.\")\n",
    "\n",
    "    # First, test the embedding functionality with a small subset\n",
    "    print(\"Testing embedding functionality with a small subset...\")\n",
    "    test_size = 3  # Small test subset\n",
    "    test_subset_ids = test_patient_ids[:test_size]\n",
    "    \n",
    "    # Create summaries for test subset\n",
    "    test_subset_summaries = {}\n",
    "    for pid in test_subset_ids:\n",
    "        test_subset_summaries[pid] = create_text_summary_from_agg(pid, X_test_agg, features_for_summary_llm)\n",
    "    \n",
    "    # Test getting embeddings for the subset\n",
    "    print(\"Testing embedding generation...\")\n",
    "    test_embeddings = {}\n",
    "    for pid in test_subset_ids:\n",
    "        embedding = get_ollama_embedding(test_subset_summaries[pid], model_name=LLM_MODEL)\n",
    "        if embedding is not None:\n",
    "            test_embeddings[pid] = embedding\n",
    "            print(f\"Successfully got embedding for patient {pid}, dimension: {len(embedding)}\")\n",
    "        else:\n",
    "            print(f\"Failed to get embedding for patient {pid}\")\n",
    "    \n",
    "    if not test_embeddings:\n",
    "        print(\"ERROR: Failed to generate test embeddings. Check Ollama server.\")\n",
    "        return\n",
    "    \n",
    "    # Prompt user for how many patients to process\n",
    "    max_patients = input(f\"Specify maximum number of patients to process (default: all {len(train_patient_ids)} train, {len(test_patient_ids)} test): \")\n",
    "    try:\n",
    "        max_patients = int(max_patients)\n",
    "        train_patient_ids = train_patient_ids[:max_patients]\n",
    "        test_patient_ids = test_patient_ids[:max_patients]\n",
    "        print(f\"Will process {len(train_patient_ids)} train and {len(test_patient_ids)} test patients\")\n",
    "    except:\n",
    "        print(f\"Using all patients: {len(train_patient_ids)} train and {len(test_patient_ids)} test\")\n",
    "    \n",
    "    # Generate summaries in parallel\n",
    "    print(\"\\nGenerating patient summaries for train set...\")\n",
    "    train_summaries = process_summaries_parallel(\n",
    "        train_patient_ids, X_train_agg, features_for_summary_llm, max_workers=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGenerating patient summaries for test set...\")\n",
    "    test_summaries = process_summaries_parallel(\n",
    "        test_patient_ids, X_test_agg, features_for_summary_llm, max_workers=5\n",
    "    )\n",
    "    \n",
    "    # Get embeddings in parallel with batching\n",
    "    print(f\"\\nGenerating LLM embeddings for train set using {LLM_MODEL}...\")\n",
    "    X_train_llm_emb_dict = get_embeddings_parallel(\n",
    "        train_summaries, model_name=LLM_MODEL, batch_size=10, max_workers=5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGenerating LLM embeddings for test set using {LLM_MODEL}...\")\n",
    "    X_test_llm_emb_dict = get_embeddings_parallel(\n",
    "        test_summaries, model_name=LLM_MODEL, batch_size=10, max_workers=5\n",
    "    )\n",
    "\n",
    "    # Filter for patients with valid embeddings\n",
    "    train_ids_with_emb = [pid for pid in train_patient_ids if pid in X_train_llm_emb_dict]\n",
    "    test_ids_with_emb = [pid for pid in test_patient_ids if pid in X_test_llm_emb_dict]\n",
    "\n",
    "    if not train_ids_with_emb or not test_ids_with_emb:\n",
    "        print(\"Error: Failed to generate sufficient LLM embeddings.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Successfully generated embeddings for {len(train_ids_with_emb)}/{len(train_patient_ids)} train patients\")\n",
    "    print(f\"Successfully generated embeddings for {len(test_ids_with_emb)}/{len(test_patient_ids)} test patients\")\n",
    "    \n",
    "    # Convert embeddings to numpy arrays for processing\n",
    "    X_train_llm_emb = np.array([X_train_llm_emb_dict[pid] for pid in train_ids_with_emb])\n",
    "    y_train_llm_emb = y_train.loc[train_ids_with_emb].values\n",
    "    \n",
    "    X_test_llm_emb = np.array([X_test_llm_emb_dict[pid] for pid in test_ids_with_emb])\n",
    "    y_test_llm_emb = y_test.loc[test_ids_with_emb].values\n",
    "    \n",
    "    # Filter out NaN labels\n",
    "    valid_train_llm_idx = ~np.isnan(y_train_llm_emb)\n",
    "    valid_test_llm_idx = ~np.isnan(y_test_llm_emb)\n",
    "\n",
    "    if np.sum(valid_train_llm_idx) < 2 or np.sum(valid_test_llm_idx) == 0:\n",
    "        print(\"Warning: Not enough valid samples for LLM embedding probe.\")\n",
    "        return\n",
    "    \n",
    "    X_train_llm_emb = X_train_llm_emb[valid_train_llm_idx]\n",
    "    y_train_llm_emb = y_train_llm_emb[valid_train_llm_idx]\n",
    "    \n",
    "    X_test_llm_emb = X_test_llm_emb[valid_test_llm_idx]\n",
    "    y_test_llm_emb = y_test_llm_emb[valid_test_llm_idx]\n",
    "    \n",
    "    print(f\"LLM Embeddings shape: Train X={X_train_llm_emb.shape}, Test X={X_test_llm_emb.shape}\")\n",
    "    \n",
    "    # Linear Probe on LLM embeddings\n",
    "    print(\"\\nTraining Linear Probe on LLM Embeddings...\")\n",
    "    probe_llm = LogisticRegression(\n",
    "        solver='liblinear', random_state=SEED, \n",
    "        max_iter=1000, C=1.0, class_weight='balanced'\n",
    "    )\n",
    "    probe_llm.fit(X_train_llm_emb, y_train_llm_emb)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred_proba_llm_probe = probe_llm.predict_proba(X_test_llm_emb)[:, 1]\n",
    "    print(f\"\\nLinear Probe Results (LLM Embeddings - {LLM_MODEL}):\")\n",
    "    auroc_llm_probe, auprc_llm_probe = calculate_metrics(\n",
    "        y_test_llm_emb, y_pred_proba_llm_probe, \n",
    "        set_name=f\"Test (LLM Emb Probe {LLM_MODEL})\"\n",
    "    )\n",
    "    \n",
    "    # Save probe model\n",
    "    os.makedirs(f\"{MODELS_DIR}/llm_probe\", exist_ok=True)\n",
    "    with open(f\"{MODELS_DIR}/llm_probe/probe_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(probe_llm, f)\n",
    "    \n",
    "    # Visualization with t-SNE\n",
    "    print(\"\\nVisualizing LLM embeddings with t-SNE...\")\n",
    "    \n",
    "    # Sample if there are too many points for effective t-SNE visualization\n",
    "    max_tsne_samples = 1000  # t-SNE can be slow with too many samples\n",
    "    if len(X_train_llm_emb) > max_tsne_samples:\n",
    "        print(f\"Sampling {max_tsne_samples} points for t-SNE visualization...\")\n",
    "        tsne_indices = np.random.choice(len(X_train_llm_emb), max_tsne_samples, replace=False)\n",
    "        X_train_llm_emb_tsne_input = X_train_llm_emb[tsne_indices]\n",
    "        y_train_llm_emb_tsne = y_train_llm_emb[tsne_indices]\n",
    "    else:\n",
    "        X_train_llm_emb_tsne_input = X_train_llm_emb\n",
    "        y_train_llm_emb_tsne = y_train_llm_emb\n",
    "    \n",
    "    # Apply t-SNE for visualization\n",
    "    tsne = TSNE(\n",
    "        n_components=2, \n",
    "        random_state=SEED, \n",
    "        perplexity=min(30, len(X_train_llm_emb_tsne_input) - 1),\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "    \n",
    "    # Apply t-SNE to training data\n",
    "    X_train_llm_tsne = tsne.fit_transform(X_train_llm_emb_tsne_input)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    if len(np.unique(y_train_llm_emb_tsne)) > 1 and min(np.bincount(y_train_llm_emb_tsne.astype(int))) >= 2:\n",
    "        silhouette = silhouette_score(X_train_llm_tsne, y_train_llm_emb_tsne)\n",
    "        print(f\"t-SNE Silhouette Score (LLM Embeddings): {silhouette:.4f}\")\n",
    "    else:\n",
    "        print(\"Cannot calculate silhouette score: insufficient class distribution\")\n",
    "    \n",
    "    # Plot t-SNE visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for label in np.unique(y_train_llm_emb_tsne):\n",
    "        indices = y_train_llm_emb_tsne == label\n",
    "        plt.scatter(\n",
    "            X_train_llm_tsne[indices, 0], X_train_llm_tsne[indices, 1],\n",
    "            label=f\"{'Survived' if int(label) == 0 else 'Died'}\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.title(f\"t-SNE Visualization of LLM Embeddings ({LLM_MODEL})\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(f\"{RESULTS_DIR}/llm_embedding_tsne.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Save embeddings for future use\n",
    "    with open(f\"{RESULTS_DIR}/llm_embeddings_train.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"embeddings\": X_train_llm_emb, \"labels\": y_train_llm_emb}, f)\n",
    "        \n",
    "    with open(f\"{RESULTS_DIR}/llm_embeddings_test.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"embeddings\": X_test_llm_emb, \"labels\": y_test_llm_emb}, f)\n",
    "    \n",
    "    # Save results\n",
    "    with open(f\"{RESULTS_DIR}/llm_embedding_results.txt\", \"w\") as f:\n",
    "        f.write(f\"LLM Model for Embeddings: {LLM_MODEL}\\n\")\n",
    "        f.write(f\"Embedding Dimension: {X_train_llm_emb.shape[1]}\\n\")\n",
    "        f.write(f\"Valid training samples: {len(X_train_llm_emb)}\\n\")\n",
    "        f.write(f\"Valid test samples: {len(X_test_llm_emb)}\\n\")\n",
    "        f.write(f\"Linear Probe AuROC: {auroc_llm_probe:.4f}\\n\")\n",
    "        f.write(f\"Linear Probe AuPRC: {auprc_llm_probe:.4f}\\n\")\n",
    "        if 'silhouette' in locals():\n",
    "            f.write(f\"t-SNE Silhouette Score: {silhouette:.4f}\\n\")\n",
    "    \n",
    "    print(f\"\\nEmbeddings successfully generated and evaluated. Results saved to {RESULTS_DIR}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_llm_embeddings()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Q4.2 execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n--- Finished Q4.2 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be660db-1b51-4e58-9520-355a861fc7fd",
   "metadata": {},
   "source": [
    "### Q4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c63ccbe-64ed-4ebf-bdb5-00b324829d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import warnings\n",
    "from chronos.models import ChronosForecaster  # You may need to install this library\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "ML_READY_PATH = 'ml_ready_data'   # Where your scaled and feature-engineered files are\n",
    "PROCESSED_PATH = 'processed_data' # Where your initial imputed files are\n",
    "MODELS_DIR = 'models'             # Directory to save trained models\n",
    "RESULTS_DIR = 'results'           # Directory to save plots and result summaries\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "STATIC_VARS = ['Age', 'Gender', 'Height', 'Weight']\n",
    "TARGET_VAR = 'In_hospital_death'\n",
    "CHRONOS_MODEL_NAME = \"chronos/chronos-t5-small\"  # Example model name (use an appropriate Chronos model)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Try to load processed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "try:\n",
    "    # Load the raw processed data with time series\n",
    "    train_data_raw = pd.read_parquet(f'{PROCESSED_PATH}/set-a.parquet')\n",
    "    test_data_raw = pd.read_parquet(f'{PROCESSED_PATH}/set-c.parquet')\n",
    "    \n",
    "    # Load the patient features (aggregated data) for the target variable and indices\n",
    "    X_train_agg = pd.read_parquet(f'{ML_READY_PATH}/patient_features-a.parquet')\n",
    "    X_test_agg = pd.read_parquet(f'{ML_READY_PATH}/patient_features-c.parquet')\n",
    "    \n",
    "    # Set index to PatientID for convenience\n",
    "    X_train_agg = X_train_agg.set_index('PatientID') if 'PatientID' in X_train_agg.columns else X_train_agg\n",
    "    X_test_agg = X_test_agg.set_index('PatientID') if 'PatientID' in X_test_agg.columns else X_test_agg\n",
    "    \n",
    "    # Extract labels\n",
    "    y_train = X_train_agg[TARGET_VAR] if TARGET_VAR in X_train_agg.columns else None\n",
    "    y_test = X_test_agg[TARGET_VAR] if TARGET_VAR in X_test_agg.columns else None\n",
    "    \n",
    "    # Identify time series variables\n",
    "    id_vars = ['PatientID', 'RecordID', 'Hour']\n",
    "    exclude_cols = STATIC_VARS + id_vars + ['ICUType', TARGET_VAR]\n",
    "    TIME_SERIES_VARS = [col for col in train_data_raw.columns if col not in exclude_cols]\n",
    "    print(f\"Found {len(TIME_SERIES_VARS)} time series variables.\")\n",
    "    \n",
    "    print(\"Successfully loaded preprocessed data.\")\n",
    "    DATA_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"Error loading preprocessed data: {e}\")\n",
    "    DATA_AVAILABLE = False\n",
    "\n",
    "# Define metrics calculation function\n",
    "def calculate_metrics(y_true, y_pred_proba, set_name=\"Test\"):\n",
    "    \"\"\"Calculates and prints AuROC and AuPRC.\"\"\"\n",
    "    try:\n",
    "        y_true, y_pred_proba = np.asarray(y_true), np.asarray(y_pred_proba)\n",
    "        if not np.all(np.isfinite(y_true)): \n",
    "            print(f\"Warning: Non-finite y_true for {set_name}. Skip.\")\n",
    "            return np.nan, np.nan\n",
    "        if not np.all(np.isfinite(y_pred_proba)): \n",
    "            print(f\"Warning: Non-finite y_pred_proba for {set_name}. Replace.\")\n",
    "            y_pred_proba = np.nan_to_num(y_pred_proba, nan=0.5, posinf=1.0, neginf=0.0)\n",
    "        if len(np.unique(y_true)) < 2: \n",
    "            print(f\"Warning: Only one class in y_true for {set_name}.\")\n",
    "            return np.nan, np.nan\n",
    "            \n",
    "        auroc = roc_auc_score(y_true, y_pred_proba)\n",
    "        auprc = average_precision_score(y_true, y_pred_proba)\n",
    "        \n",
    "        print(f\"{set_name} AuROC: {auroc:.4f}\")\n",
    "        print(f\"{set_name} AuPRC: {auprc:.4f}\")\n",
    "        return auroc, auprc\n",
    "    except ValueError as e: \n",
    "        print(f\"Metrics Error for {set_name}: {e}\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "# Function to preprocess time series data for Chronos\n",
    "def preprocess_for_chronos(data, patient_id, variable):\n",
    "    \"\"\"\n",
    "    Preprocess a single time series variable for a single patient to feed into Chronos.\n",
    "    Returns a numpy array of shape (sequence_length,) with the time series values.\n",
    "    \"\"\"\n",
    "    # Get data for this patient\n",
    "    patient_data = data[data['PatientID'] == patient_id].copy()\n",
    "    \n",
    "    # Sort by hour\n",
    "    patient_data = patient_data.sort_values('Hour')\n",
    "    \n",
    "    # Extract the variable values\n",
    "    values = patient_data[variable].values\n",
    "    \n",
    "    # Handle missing values (NaN) by forward filling then backward filling\n",
    "    mask = np.isnan(values)\n",
    "    idx = np.where(~mask, np.arange(len(mask)), 0)\n",
    "    np.maximum.accumulate(idx, out=idx)\n",
    "    values = values[idx]\n",
    "    \n",
    "    # If all values are NaN, return zeros\n",
    "    if np.all(np.isnan(values)):\n",
    "        return np.zeros(len(values))\n",
    "    \n",
    "    # Ensure the sequence is long enough (pad if needed)\n",
    "    min_length = 10  # Minimum length for the model\n",
    "    if len(values) < min_length:\n",
    "        padding = np.zeros(min_length - len(values))\n",
    "        values = np.concatenate([values, padding])\n",
    "    \n",
    "    return values\n",
    "\n",
    "# Load Chronos model (this is a placeholder, you'll need to implement according to Chronos API)\n",
    "def load_chronos_model():\n",
    "    \"\"\"\n",
    "    Load a pre-trained Chronos time-series foundation model.\n",
    "    Returns a model that can be used to get embeddings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading pre-trained Chronos model: {CHRONOS_MODEL_NAME}\")\n",
    "        \n",
    "        # This is a placeholder - you'll need to implement based on Chronos documentation\n",
    "        # For example:\n",
    "        # model = ChronosForecaster.from_pretrained(CHRONOS_MODEL_NAME)\n",
    "        \n",
    "        # For now, we'll simulate a model with a simple function to generate embeddings\n",
    "        def mock_chronos_embedding_function(time_series):\n",
    "            \"\"\"Mock function to simulate Chronos embeddings for testing\"\"\"\n",
    "            # For testing, we'll return random embeddings of dimension 128\n",
    "            return np.random.randn(128)\n",
    "        \n",
    "        return mock_chronos_embedding_function\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Chronos model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to get Chronos embeddings for a patient's time series\n",
    "def get_chronos_embeddings(data, patient_id, variables, chronos_model):\n",
    "    \"\"\"\n",
    "    Get Chronos embeddings for all specified variables for a single patient.\n",
    "    Returns a dictionary of variable_name -> embedding.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    \n",
    "    for var in variables:\n",
    "        try:\n",
    "            # Preprocess the time series for this variable\n",
    "            time_series = preprocess_for_chronos(data, patient_id, var)\n",
    "            \n",
    "            # Get embedding from Chronos model\n",
    "            # Note: The actual API call will depend on the Chronos library's interface\n",
    "            embedding = chronos_model(time_series)\n",
    "            \n",
    "            # Store the embedding\n",
    "            embeddings[var] = embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting embedding for patient {patient_id}, variable {var}: {e}\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Function to process patients in parallel\n",
    "def process_patients_in_parallel(patient_ids, data, variables, chronos_model, max_workers=5):\n",
    "    \"\"\"Process patient embeddings in parallel using thread pool\"\"\"\n",
    "    all_embeddings = {}\n",
    "    \n",
    "    def process_patient(pid):\n",
    "        return pid, get_chronos_embeddings(data, pid, variables, chronos_model)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_patient, pid): pid for pid in patient_ids}\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing patients\"):\n",
    "            pid = futures[future]\n",
    "            try:\n",
    "                pid, embeddings = future.result()\n",
    "                all_embeddings[pid] = embeddings\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing patient {pid}: {e}\")\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# Function to create a single embedding per patient by averaging across variables\n",
    "def average_embeddings(patient_embeddings):\n",
    "    \"\"\"\n",
    "    Create a single embedding per patient by averaging across all variables.\n",
    "    Returns a dictionary of patient_id -> average_embedding.\n",
    "    \"\"\"\n",
    "    averaged_embeddings = {}\n",
    "    \n",
    "    for patient_id, var_embeddings in patient_embeddings.items():\n",
    "        if not var_embeddings:  # Skip if no embeddings for this patient\n",
    "            continue\n",
    "            \n",
    "        # Convert all embeddings to numpy arrays and stack them\n",
    "        embeddings_list = [emb for var, emb in var_embeddings.items() if emb is not None]\n",
    "        \n",
    "        if not embeddings_list:  # Skip if no valid embeddings\n",
    "            continue\n",
    "            \n",
    "        # Stack and average across variables\n",
    "        stacked_embeddings = np.stack(embeddings_list)\n",
    "        avg_embedding = np.mean(stacked_embeddings, axis=0)\n",
    "        \n",
    "        averaged_embeddings[patient_id] = avg_embedding\n",
    "    \n",
    "    return averaged_embeddings\n",
    "\n",
    "# Channel aggregation neural network\n",
    "class ChannelAggregator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=1, num_variables=None):\n",
    "        super(ChannelAggregator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Variable attention mechanism\n",
    "        if num_variables:\n",
    "            self.attention = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, 1),\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, var_embeddings=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        If var_embeddings is provided (shape: batch_size x num_variables x embedding_dim),\n",
    "        use attention mechanism to aggregate variables.\n",
    "        If x is provided (shape: batch_size x embedding_dim), \n",
    "        it's assumed to be already aggregated embeddings.\n",
    "        \"\"\"\n",
    "        if var_embeddings is not None:\n",
    "            # Apply attention to variable embeddings\n",
    "            batch_size, num_vars, emb_dim = var_embeddings.shape\n",
    "            \n",
    "            # Reshape for attention\n",
    "            var_embeddings_flat = var_embeddings.view(-1, emb_dim)\n",
    "            attention_flat = self.attention(var_embeddings_flat)\n",
    "            attention_weights = attention_flat.view(batch_size, num_vars, 1)\n",
    "            \n",
    "            # Apply attention weights\n",
    "            weighted_embeddings = var_embeddings * attention_weights\n",
    "            x = weighted_embeddings.sum(dim=1)  # Sum across variables\n",
    "        \n",
    "        # Apply prediction layers\n",
    "        return self.predictor(x)\n",
    "\n",
    "# Function to train the channel aggregator model\n",
    "def train_channel_aggregator(train_embeddings, train_labels, val_embeddings=None, val_labels=None, \n",
    "                            batch_size=32, epochs=50, learning_rate=0.001, patience=5):\n",
    "    \"\"\"\n",
    "    Train a neural network to aggregate channel embeddings.\n",
    "    \n",
    "    Args:\n",
    "        train_embeddings: Dictionary {patient_id -> {variable -> embedding}}\n",
    "        train_labels: Dictionary {patient_id -> label}\n",
    "        val_embeddings: Optional validation embeddings\n",
    "        val_labels: Optional validation labels\n",
    "        \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    # Process embeddings into tensor format\n",
    "    patient_ids = list(train_embeddings.keys())\n",
    "    var_names = list(train_embeddings[patient_ids[0]].keys())\n",
    "    embedding_dim = train_embeddings[patient_ids[0]][var_names[0]].shape[0]\n",
    "    \n",
    "    # Create tensor datasets\n",
    "    X_train_tensors = []\n",
    "    y_train_tensors = []\n",
    "    \n",
    "    for pid in patient_ids:\n",
    "        if pid not in train_labels:\n",
    "            continue\n",
    "            \n",
    "        # Stack variable embeddings for this patient\n",
    "        var_embeddings = []\n",
    "        for var in var_names:\n",
    "            if var in train_embeddings[pid]:\n",
    "                var_embeddings.append(train_embeddings[pid][var])\n",
    "        \n",
    "        if not var_embeddings:\n",
    "            continue\n",
    "            \n",
    "        # Stack and append\n",
    "        stacked_embeddings = np.stack(var_embeddings)\n",
    "        X_train_tensors.append(stacked_embeddings)\n",
    "        y_train_tensors.append(train_labels[pid])\n",
    "    \n",
    "    # Convert to pytorch tensors\n",
    "    X_train = torch.tensor(np.stack(X_train_tensors), dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train_tensors, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    num_variables = len(var_names)\n",
    "    model = ChannelAggregator(\n",
    "        input_dim=embedding_dim, \n",
    "        hidden_dim=64, \n",
    "        output_dim=1, \n",
    "        num_variables=num_variables\n",
    "    )\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(None, batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * batch_x.size(0)\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        if val_embeddings and val_labels:\n",
    "            # Process validation data (similar to training data)\n",
    "            # ... (code would be similar to training data processing)\n",
    "            val_loss = 0.0  # Compute validation loss\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stop_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), f\"{MODELS_DIR}/channel_aggregator.pt\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "        else:\n",
    "            # If no validation data, save the model periodically\n",
    "            if epoch % 10 == 0:\n",
    "                torch.save(model.state_dict(), f\"{MODELS_DIR}/channel_aggregator_epoch{epoch}.pt\")\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}')\n",
    "    \n",
    "    # Load best model if validation was used\n",
    "    if val_embeddings and val_labels and os.path.exists(f\"{MODELS_DIR}/channel_aggregator.pt\"):\n",
    "        model.load_state_dict(torch.load(f\"{MODELS_DIR}/channel_aggregator.pt\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to get predictions from the channel aggregator model\n",
    "def predict_with_channel_aggregator(model, embeddings, device=None):\n",
    "    \"\"\"\n",
    "    Get predictions from the trained channel aggregator model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ChannelAggregator model\n",
    "        embeddings: Dictionary {patient_id -> {variable -> embedding}}\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary {patient_id -> prediction}\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    # Process patients in batches\n",
    "    patient_ids = list(embeddings.keys())\n",
    "    var_names = list(next(iter(embeddings.values())).keys())\n",
    "    \n",
    "    for i in range(0, len(patient_ids), 32):  # Process in batches of 32\n",
    "        batch_pids = patient_ids[i:i+32]\n",
    "        batch_embeddings = []\n",
    "        \n",
    "        for pid in batch_pids:\n",
    "            # Stack variable embeddings for this patient\n",
    "            var_embeddings = []\n",
    "            for var in var_names:\n",
    "                if var in embeddings[pid]:\n",
    "                    var_embeddings.append(embeddings[pid][var])\n",
    "            \n",
    "            if not var_embeddings:\n",
    "                continue\n",
    "                \n",
    "            # Stack and append\n",
    "            stacked_embeddings = np.stack(var_embeddings)\n",
    "            batch_embeddings.append(stacked_embeddings)\n",
    "        \n",
    "        if not batch_embeddings:\n",
    "            continue\n",
    "            \n",
    "        # Convert to pytorch tensor\n",
    "        X_batch = torch.tensor(np.stack(batch_embeddings), dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(None, X_batch)\n",
    "            outputs = outputs.cpu().numpy().flatten()\n",
    "        \n",
    "        # Store predictions\n",
    "        for j, pid in enumerate(batch_pids[:len(outputs)]):\n",
    "            predictions[pid] = outputs[j]\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Main execution function for Q4.3\n",
    "def run_time_series_foundation_models():\n",
    "    \"\"\"Execute Q4.3: Using time-series foundation models\"\"\"\n",
    "    print(\"\\n--- Q4.3: Using time-series foundation models ---\")\n",
    "    \n",
    "    # Select a subset of key time series variables (to reduce computation time)\n",
    "    selected_variables = [\n",
    "        'HR', 'RespRate', 'SysABP', 'DiasABP', 'MAP', 'Temp', 'GCS',\n",
    "        'PaO2', 'SaO2', 'FiO2', 'Lactate', 'Creatinine', 'Glucose'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only include variables that exist in our dataset\n",
    "    available_variables = [var for var in selected_variables if var in TIME_SERIES_VARS]\n",
    "    print(f\"Using {len(available_variables)} available time series variables: {available_variables}\")\n",
    "    \n",
    "    # Load Chronos model\n",
    "    chronos_model = load_chronos_model()\n",
    "    if chronos_model is None:\n",
    "        print(\"Failed to load Chronos model. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Get patient IDs\n",
    "    train_patient_ids = train_data_raw['PatientID'].unique()\n",
    "    test_patient_ids = test_data_raw['PatientID'].unique()\n",
    "    \n",
    "    # For testing/development, use a smaller subset\n",
    "    max_patients = input(f\"Specify maximum number of patients to process (default: all {len(train_patient_ids)} train, {len(test_patient_ids)} test): \")\n",
    "    try:\n",
    "        max_patients = int(max_patients)\n",
    "        train_patient_ids = train_patient_ids[:max_patients]\n",
    "        test_patient_ids = test_patient_ids[:max_patients]\n",
    "        print(f\"Will process {len(train_patient_ids)} train and {len(test_patient_ids)} test patients\")\n",
    "    except:\n",
    "        print(f\"Using all patients: {len(train_patient_ids)} train and {len(test_patient_ids)} test\")\n",
    "    \n",
    "    # Generate Chronos embeddings for each patient's time series\n",
    "    print(\"\\nGenerating Chronos embeddings for training set...\")\n",
    "    train_embeddings = process_patients_in_parallel(\n",
    "        train_patient_ids, train_data_raw, available_variables, chronos_model\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGenerating Chronos embeddings for test set...\")\n",
    "    test_embeddings = process_patients_in_parallel(\n",
    "        test_patient_ids, test_data_raw, available_variables, chronos_model\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully generated embeddings for {len(train_embeddings)} train patients and {len(test_embeddings)} test patients\")\n",
    "    \n",
    "    # --- Part 1: Simple averaging approach ---\n",
    "    print(\"\\n--- Part 1: Average embeddings across variables ---\")\n",
    "    \n",
    "    # Average embeddings across variables for each patient\n",
    "    train_avg_embeddings = average_embeddings(train_embeddings)\n",
    "    test_avg_embeddings = average_embeddings(test_embeddings)\n",
    "    \n",
    "    print(f\"Averaged embeddings for {len(train_avg_embeddings)} train patients and {len(test_avg_embeddings)} test patients\")\n",
    "    \n",
    "    # Convert to numpy arrays for the linear probe\n",
    "    train_ids = list(train_avg_embeddings.keys())\n",
    "    test_ids = list(test_avg_embeddings.keys())\n",
    "    \n",
    "    X_train = np.array([train_avg_embeddings[pid] for pid in train_ids])\n",
    "    y_train = np.array([y_train.loc[pid] if pid in y_train.index else np.nan for pid in train_ids])\n",
    "    \n",
    "    X_test = np.array([test_avg_embeddings[pid] for pid in test_ids])\n",
    "    y_test_vals = np.array([y_test.loc[pid] if pid in y_test.index else np.nan for pid in test_ids])\n",
    "    \n",
    "    # Remove NaN labels\n",
    "    valid_train_idx = ~np.isnan(y_train)\n",
    "    valid_test_idx = ~np.isnan(y_test_vals)\n",
    "    \n",
    "    X_train_clean = X_train[valid_train_idx]\n",
    "    y_train_clean = y_train[valid_train_idx]\n",
    "    X_test_clean = X_test[valid_test_idx]\n",
    "    y_test_clean = y_test_vals[valid_test_idx]\n",
    "    \n",
    "    print(f\"Clean data shapes: Train X={X_train_clean.shape}, Test X={X_test_clean.shape}\")\n",
    "    \n",
    "    # Train linear probe\n",
    "    print(\"\\nTraining linear probe on averaged Chronos embeddings...\")\n",
    "    probe = LogisticRegression(\n",
    "        solver='liblinear', random_state=SEED, \n",
    "        max_iter=1000, C=1.0, class_weight='balanced'\n",
    "    )\n",
    "    probe.fit(X_train_clean, y_train_clean)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred_proba = probe.predict_proba(X_test_clean)[:, 1]\n",
    "    \n",
    "    print(f\"\\nLinear Probe Results (Averaged Chronos Embeddings):\")\n",
    "    auroc_avg, auprc_avg = calculate_metrics(\n",
    "        y_test_clean, y_pred_proba, set_name=\"Test (Avg Chronos Embeddings)\"\n",
    "    )\n",
    "    \n",
    "    # Save probe model\n",
    "    os.makedirs(f\"{MODELS_DIR}/chronos_probe\", exist_ok=True)\n",
    "    with open(f\"{MODELS_DIR}/chronos_probe/avg_probe_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(probe, f)\n",
    "    \n",
    "    # --- Part 2: Neural Network for channel aggregation ---\n",
    "    print(\"\\n--- Part 2: Neural network for channel aggregation ---\")\n",
    "    \n",
    "    # Convert labels to dictionary format for the training function\n",
    "    train_labels_dict = {pid: y_train.loc[pid] for pid in train_ids if pid in y_train.index and not np.isnan(y_train.loc[pid])}\n",
    "    test_labels_dict = {pid: y_test.loc[pid] for pid in test_ids if pid in y_test.index and not np.isnan(y_test.loc[pid])}\n",
    "    \n",
    "    # Train channel aggregator model\n",
    "    print(\"\\nTraining channel aggregator neural network...\")\n",
    "    aggregator_model = train_channel_aggregator(\n",
    "        train_embeddings, train_labels_dict,\n",
    "        batch_size=16, epochs=30, learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    print(\"\\nGenerating predictions with channel aggregator model...\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    predictions = predict_with_channel_aggregator(aggregator_model, test_embeddings, device)\n",
    "    \n",
    "    # Extract test patients with valid labels for evaluation\n",
    "    test_pids_for_eval = list(test_labels_dict.keys())\n",
    "    y_true_nn = np.array([test_labels_dict[pid] for pid in test_pids_for_eval if pid in predictions])\n",
    "    y_pred_nn = np.array([predictions[pid] for pid in test_pids_for_eval if pid in predictions])\n",
    "    \n",
    "    print(f\"\\nChannel Aggregator Neural Network Results:\")\n",
    "    auroc_nn, auprc_nn = calculate_metrics(\n",
    "        y_true_nn, y_pred_nn, set_name=\"Test (Channel Aggregator NN)\"\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    with open(f\"{RESULTS_DIR}/chronos_results.txt\", \"w\") as f:\n",
    "        f.write(\"Chronos Time Series Foundation Model Results\\n\")\n",
    "        f.write(\"===========================================\\n\\n\")\n",
    "        f.write(\"1. Simple Averaging Approach\\n\")\n",
    "        f.write(f\"Number of training patients: {len(X_train_clean)}\\n\")\n",
    "        f.write(f\"Number of test patients: {len(X_test_clean)}\\n\")\n",
    "        f.write(f\"AuROC: {auroc_avg:.4f}\\n\")\n",
    "        f.write(f\"AuPRC: {auprc_avg:.4f}\\n\\n\")\n",
    "        f.write(\"2. Neural Network Channel Aggregation\\n\")\n",
    "        f.write(f\"Number of training patients: {len(train_labels_dict)}\\n\")\n",
    "        f.write(f\"Number of test patients with predictions: {len(y_true_nn)}\\n\")\n",
    "        f.write(f\"AuROC: {auroc_nn:.4f}\\n\")\n",
    "        f.write(f\"AuPRC: {auprc_nn:.4f}\\n\")\n",
    "    \n",
    "    print(f\"\\nResults saved to {RESULTS_DIR}/chronos_results.txt\")\n",
    "    print(\"\\n--- Finished Q4.3 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_time_series_foundation_models()\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing Q4.3: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6350a584-be08-4e9e-9b94-345bf4d6eea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

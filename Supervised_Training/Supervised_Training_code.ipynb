{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1 Classic Machine Learning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tsfresh.feature_extraction import extract_features, MinimalFCParameters\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import make_forecasting_frame\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - AUROC: 0.8468, AUPRC: 0.5152\n",
      "Random Forest - AUROC: 0.8346, AUPRC: 0.5078\n",
      "SVM - AUROC: 0.8309, AUPRC: 0.4873\n"
     ]
    }
   ],
   "source": [
    "# === PART 1 OF Q2.1 ===\n",
    "# Load preprocessed datasets\n",
    "df_a = pd.read_parquet('C:/User/ETH/2024-2025/MA2/ML for human health/Project 1/ML-For-Healthcare-25/ml_ready_data/set-a-scaled.parquet')\n",
    "df_b = pd.read_parquet('C:/User/ETH/2024-2025/MA2/ML for human health/Project 1/ML-For-Healthcare-25/ml_ready_data/set-b-scaled.parquet')\n",
    "df_c = pd.read_parquet('C:/User/ETH/2024-2025/MA2/ML for human health/Project 1/ML-For-Healthcare-25/ml_ready_data/set-c-scaled.parquet')  # Test set\n",
    "\n",
    "df_train = df_a\n",
    "df_validate = df_b\n",
    "df_test = df_c  # Test set is set-c\n",
    "\n",
    "# Define static and dynamic variables\n",
    "static_vars = ['Age', 'Gender', 'Height', 'Weight']\n",
    "time_series_vars = [col for col in df_train.columns if col not in static_vars + ['Hour', 'PatientID', 'RecordID', 'In_hospital_death']]\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(df):\n",
    "    features = df.groupby('PatientID')[time_series_vars].agg(['mean', 'max', 'last'])\n",
    "    features.columns = ['_'.join(col) for col in features.columns]\n",
    "    \n",
    "    # Add static variables\n",
    "    static_data = df.groupby('PatientID')[static_vars].last()\n",
    "    features = features.merge(static_data, left_index=True, right_index=True)\n",
    "    \n",
    "    # Add labels\n",
    "    labels = df.groupby('PatientID')['In_hospital_death'].last()\n",
    "    features['In_hospital_death'] = labels\n",
    "    return features.reset_index()\n",
    "\n",
    "# Extract features\n",
    "df_train_features = extract_features(df_train)\n",
    "df_test_features = extract_features(df_test)\n",
    "\n",
    "# Prepare data for ML\n",
    "X_train = df_train_features.drop(columns=['PatientID', 'In_hospital_death'])\n",
    "y_train = df_train_features['In_hospital_death']\n",
    "X_test = df_test_features.drop(columns=['PatientID', 'In_hospital_death'])\n",
    "y_test = df_test_features['In_hospital_death']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train classifiers\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(model, X_test, y_test, name):\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    auprc = average_precision_score(y_test, y_pred)\n",
    "    print(f\"{name} - AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "# Report performance\n",
    "evaluate_model(logreg, X_test, y_test, 'Logistic Regression')\n",
    "evaluate_model(rf, X_test, y_test, 'Random Forest')\n",
    "evaluate_model(svm_model, X_test, y_test, 'SVM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 168000/168000 [00:46<00:00, 3601.17it/s]\n",
      "Feature Extraction: 100%|██████████| 168000/168000 [00:49<00:00, 3364.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant features selected by tsfresh:\n",
      "['GCS__median', 'BUN__median', 'BUN__sum_values', 'BUN__mean', 'GCS__maximum', 'GCS__sum_values', 'GCS__mean', 'BUN__maximum', 'BUN__minimum', 'Urine__mean', 'Urine__sum_values', 'GCS__root_mean_square', 'Urine__median', 'Urine__standard_deviation', 'Urine__variance', 'Urine__maximum', 'Urine__absolute_maximum', 'Creatinine__maximum', 'Urine__root_mean_square', 'Temp__root_mean_square', 'pH__root_mean_square', 'Creatinine__median', 'ALP__maximum', 'Creatinine__sum_values', 'Creatinine__mean', 'Creatinine__absolute_maximum', 'pH__absolute_maximum', 'BUN__absolute_maximum', 'PaCO2__root_mean_square', 'HCO3__minimum', 'Creatinine__root_mean_square', 'Bilirubin__absolute_maximum', 'pH__standard_deviation', 'pH__variance', 'PaCO2__minimum', 'Na__root_mean_square', 'Bilirubin__root_mean_square', 'Bilirubin__maximum', 'AST__root_mean_square', 'AST__absolute_maximum', 'Glucose__absolute_maximum', 'BUN__root_mean_square', 'Albumin__minimum', 'Creatinine__minimum', 'Age__sum_values', 'Age__median', 'Age__mean', 'Age__maximum', 'Age__minimum', 'AST__standard_deviation', 'AST__variance', 'Bilirubin__variance', 'Bilirubin__standard_deviation', 'ALP__absolute_maximum', 'Glucose__maximum', 'Na__absolute_maximum', 'ALT__absolute_maximum', 'ALP__root_mean_square', 'Glucose__root_mean_square', 'ALP__variance', 'ALP__standard_deviation', 'ALT__root_mean_square', 'Lactate__standard_deviation', 'Lactate__variance', 'Lactate__maximum', 'HCO3__mean', 'HCO3__sum_values', 'ALT__variance', 'ALT__standard_deviation', 'HCO3__root_mean_square', 'PaCO2__absolute_maximum', 'Glucose__standard_deviation', 'Glucose__variance', 'Lactate__absolute_maximum', 'PaCO2__standard_deviation', 'PaCO2__variance', 'Lactate__root_mean_square', 'HCO3__median', 'HCO3__absolute_maximum', 'BUN__standard_deviation', 'BUN__variance', 'FiO2__root_mean_square', 'Creatinine__variance', 'Creatinine__standard_deviation', 'PaCO2__median', 'PaCO2__sum_values', 'PaCO2__mean', 'Glucose__sum_values', 'Glucose__mean', 'HR__root_mean_square', 'WBC__absolute_maximum', 'HR__absolute_maximum', 'RespRate__minimum', 'AST__maximum', 'Albumin__absolute_maximum', 'NISysABP__root_mean_square', 'TroponinT__variance', 'TroponinT__standard_deviation', 'Temp__absolute_maximum', 'Albumin__root_mean_square', 'FiO2__variance', 'FiO2__standard_deviation', 'TroponinT__absolute_maximum', 'TroponinT__root_mean_square', 'Temp__variance', 'Temp__standard_deviation', 'Albumin__variance', 'Albumin__standard_deviation', 'PaO2__minimum', 'WBC__root_mean_square', 'Temp__minimum', 'NIMAP__median', 'FiO2__absolute_maximum', 'RespRate__absolute_maximum', 'NIDiasABP__median', 'RespRate__variance', 'RespRate__standard_deviation', 'Glucose__median', 'RespRate__maximum', 'RespRate__root_mean_square', 'TroponinI__absolute_maximum', 'TroponinI__root_mean_square', 'ALP__sum_values', 'ALP__mean', 'HCO3__maximum', 'Na__standard_deviation', 'Na__variance', 'Platelets__root_mean_square', 'ALT__maximum', 'Urine__minimum', 'ALP__median', 'NIDiasABP__mean', 'NIDiasABP__sum_values', 'pH__minimum', 'TroponinI__standard_deviation', 'TroponinI__variance', 'NIMAP__sum_values', 'NIMAP__mean', 'K__absolute_maximum', 'HCO3__standard_deviation', 'HCO3__variance', 'Platelets__absolute_maximum', 'Temp__mean', 'Temp__sum_values', 'NISysABP__minimum', 'FiO2__maximum', 'GCS__absolute_maximum', 'GCS__minimum', 'Lactate__mean', 'Lactate__sum_values', 'HR__maximum', 'K__root_mean_square', 'NIMAP__root_mean_square', 'Temp__median', 'NIMAP__minimum', 'Lactate__median', 'Albumin__sum_values', 'Albumin__mean', 'Albumin__median', 'Bilirubin__mean', 'Bilirubin__sum_values', 'WBC__standard_deviation', 'WBC__variance', 'Bilirubin__median', 'NISysABP__absolute_maximum', 'NISysABP__sum_values', 'NISysABP__mean', 'NISysABP__median', 'WBC__maximum', 'SysABP__minimum', 'SysABP__absolute_maximum', 'HR__variance', 'HR__standard_deviation', 'MAP__minimum', 'Mg__maximum', 'TroponinT__minimum', 'K__variance', 'K__standard_deviation', 'NIDiasABP__minimum', 'Na__maximum', 'TroponinT__maximum', 'NIDiasABP__root_mean_square', 'SysABP__root_mean_square', 'DiasABP__minimum', 'ICUType__minimum', 'ICUType__absolute_maximum', 'ICUType__root_mean_square', 'ICUType__mean', 'ICUType__median', 'ICUType__sum_values', 'ICUType__maximum', 'K__maximum', 'TroponinI__minimum', 'WBC__mean', 'WBC__sum_values', 'HR__sum_values', 'HR__mean', 'Mg__root_mean_square', 'SysABP__standard_deviation', 'SysABP__variance', 'Platelets__minimum', 'Mg__median', 'WBC__median', 'pH__maximum', 'HR__median', 'TroponinI__maximum', 'ALT__minimum', 'FiO2__sum_values', 'FiO2__mean', 'Mg__sum_values', 'Mg__mean', 'Height__minimum', 'Height__maximum', 'Height__mean', 'Height__sum_values', 'Height__median', 'Platelets__variance', 'Platelets__standard_deviation', 'PaO2__root_mean_square', 'MAP__absolute_maximum', 'Platelets__median', 'DiasABP__absolute_maximum', 'Age', 'Gender', 'Height', 'Weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - AUROC: 0.8245, AUPRC: 0.4745\n",
      "Random Forest - AUROC: 0.8318, AUPRC: 0.4798\n",
      "SVM - AUROC: 0.7929, AUPRC: 0.4369\n"
     ]
    }
   ],
   "source": [
    "from tsfresh import extract_relevant_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "# === PART 2 OF Q2.1 ===\n",
    "\n",
    "# Extract features with tsfresh\n",
    "df_train = df_a\n",
    "df_test = df_c  # Test set is set-c\n",
    "\n",
    "# Define static and dynamic variables\n",
    "static_vars = ['Age', 'Gender', 'Height', 'Weight']\n",
    "time_series_vars = [col for col in df_train.columns if col not in static_vars + ['Hour', 'PatientID', 'RecordID', 'In_hospital_death']]\n",
    "\n",
    "# Extract relevant features with tsfresh\n",
    "def extract_relevant_tsfresh_features(df, y, time_series_vars, static_vars):\n",
    "    # Extract relevant time-series features using tsfresh\n",
    "    relevant_features = extract_relevant_features(\n",
    "        df[time_series_vars + static_vars + ['PatientID', 'Hour']],  # Include necessary columns\n",
    "        y,\n",
    "        column_id='PatientID',  # ID column for each patient\n",
    "        column_sort='Hour',  # Time-related sorting column\n",
    "        default_fc_parameters=MinimalFCParameters(),  # Set of features to extract\n",
    "        ml_task='classification',  # Specify the task type\n",
    "        n_jobs=1  # Adjust as needed for parallel processing\n",
    "    )\n",
    "    \n",
    "    # Add static variables (like Age, Gender, Height, Weight) to the features\n",
    "    static_data = df.groupby('PatientID')[static_vars].last()  # Get last static data for each patient\n",
    "    relevant_features = relevant_features.merge(static_data, left_index=True, right_index=True)\n",
    "    \n",
    "    return relevant_features\n",
    "\n",
    "# Prepare target variable for training and testing\n",
    "y_train = df_train.groupby('PatientID')['In_hospital_death'].last()\n",
    "y_test = df_test.groupby('PatientID')['In_hospital_death'].last()\n",
    "\n",
    "# Extract relevant features for training set\n",
    "X_train = extract_relevant_tsfresh_features(df_train, y_train, time_series_vars, static_vars)\n",
    "\n",
    "# Extract all features for the test set\n",
    "X_test_all_features = extract_features(\n",
    "    df_test[time_series_vars + static_vars + ['PatientID', 'Hour']],\n",
    "    column_id='PatientID',\n",
    "    column_sort='Hour',\n",
    "    default_fc_parameters=MinimalFCParameters(),\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# Impute missing values in the test set\n",
    "impute(X_test_all_features)\n",
    "\n",
    "# Add static variables to the test set\n",
    "static_data_test = df_test.groupby('PatientID')[static_vars].last()\n",
    "X_test_all_features = X_test_all_features.merge(static_data_test, left_index=True, right_index=True)\n",
    "\n",
    "# Align test features with training features\n",
    "X_test = X_test_all_features[X_train.columns]  # Select only the features present in X_train\n",
    "\n",
    "# Print the relevant features selected by tsfresh BEFORE standardization\n",
    "print(\"Relevant features selected by tsfresh:\")\n",
    "print(X_train.columns.tolist())\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train classifiers\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(model, X_test, y_test, name):\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    auprc = average_precision_score(y_test, y_pred)\n",
    "    print(f\"{name} - AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "# Report performance\n",
    "evaluate_model(logreg, X_test, y_test, 'Logistic Regression')\n",
    "evaluate_model(rf, X_test, y_test, 'Random Forest')\n",
    "evaluate_model(svm_model, X_test, y_test, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.2 Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.4042\n",
      "Epoch [2/20], Loss: 0.3443\n",
      "Epoch [3/20], Loss: 0.3376\n",
      "Epoch [4/20], Loss: 0.3208\n",
      "Epoch [5/20], Loss: 0.3071\n",
      "Epoch [6/20], Loss: 0.2790\n",
      "Epoch [7/20], Loss: 0.2689\n",
      "Epoch [8/20], Loss: 0.2648\n",
      "Epoch [9/20], Loss: 0.2420\n",
      "Epoch [10/20], Loss: 0.2425\n",
      "Epoch [11/20], Loss: 0.1230\n",
      "Epoch [12/20], Loss: 0.1588\n",
      "Epoch [13/20], Loss: 0.1638\n",
      "Epoch [14/20], Loss: 0.1365\n",
      "Epoch [15/20], Loss: 0.1525\n",
      "Epoch [16/20], Loss: 0.1138\n",
      "Epoch [17/20], Loss: 0.0459\n",
      "Epoch [18/20], Loss: 0.0530\n",
      "Epoch [19/20], Loss: 0.0760\n",
      "Epoch [20/20], Loss: 0.0319\n",
      "LSTM Model - Test Set C Performance: AUROC: 0.6541, AUPRC: 0.2281\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "# === Implementing LSTM ===\n",
    "\n",
    "# === Prepare Data for LSTM ===\n",
    "def prepare_lstm_data(df, time_series_vars, static_vars):\n",
    "    time_series_data = df.groupby('PatientID')[time_series_vars].apply(lambda x: x.values)\n",
    "    static_data = df.groupby('PatientID')[static_vars].last()\n",
    "    labels = df.groupby('PatientID')['In_hospital_death'].last()\n",
    "\n",
    "    # Pad sequences to ensure uniform length\n",
    "    max_timesteps = max(time_series_data.apply(len))\n",
    "    time_series_data = np.array([np.pad(x, ((0, max_timesteps - len(x)), (0, 0)), mode='constant') for x in time_series_data])\n",
    "\n",
    "    return time_series_data, static_data.values, labels.values\n",
    "\n",
    "# === Prepare Training and Test Data ===\n",
    "X_train_ts, X_train_static, y_train = prepare_lstm_data(df_a, time_series_vars, static_vars)\n",
    "X_validate_ts, X_validate_static, y_validate = prepare_lstm_data(df_b, time_series_vars, static_vars)\n",
    "X_test_ts, X_test_static, y_test = prepare_lstm_data(df_c, time_series_vars, static_vars)\n",
    "\n",
    "# === Standardize Features ===\n",
    "scaler_static = StandardScaler()\n",
    "X_train_static = scaler_static.fit_transform(X_train_static)\n",
    "X_test_static = scaler_static.transform(X_test_static)\n",
    "\n",
    "scaler_ts = StandardScaler()\n",
    "X_train_ts = np.array([scaler_ts.fit_transform(x) for x in X_train_ts])\n",
    "X_test_ts = np.array([scaler_ts.transform(x) for x in X_test_ts])\n",
    "\n",
    "# === Convert to PyTorch Tensors ===\n",
    "X_train_ts_tensor = torch.tensor(X_train_ts, dtype=torch.float32)\n",
    "X_test_ts_tensor = torch.tensor(X_test_ts, dtype=torch.float32)\n",
    "X_train_static_tensor = torch.tensor(X_train_static, dtype=torch.float32)\n",
    "X_test_static_tensor = torch.tensor(X_test_static, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# === Define the LSTM Model ===\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_shape_ts, input_shape_static):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_shape_ts[1], hidden_size=64, batch_first=True, dropout=0.3)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=32, batch_first=True, dropout=0.3)\n",
    "        self.fc1 = nn.Linear(32 + input_shape_static, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_ts, x_static):\n",
    "        lstm_out, (hn, cn) = self.lstm1(x_ts)\n",
    "        lstm_out, (hn, cn) = self.lstm2(lstm_out)\n",
    "        last_lstm_output = lstm_out[:, -1, :]  # Use the last time-step output\n",
    "\n",
    "        # Concatenate static data with LSTM output\n",
    "        combined_input = torch.cat((last_lstm_output, x_static), dim=1)\n",
    "        x = torch.relu(self.fc1(combined_input))\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# === Initialize and Train the Model ===\n",
    "input_shape_ts = (X_train_ts.shape[1], X_train_ts.shape[2])  # (timesteps, features)\n",
    "input_shape_static = X_train_static.shape[1]  # Number of static features\n",
    "\n",
    "model = LSTMModel(input_shape_ts, input_shape_static)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# === Training Loop ===\n",
    "def train_model(model, X_train_ts, X_train_static, y_train, epochs=20, batch_size=32):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        # Create batches\n",
    "        for i in range(0, len(X_train_ts), batch_size):\n",
    "            batch_ts = X_train_ts[i:i+batch_size]\n",
    "            batch_static = X_train_static[i:i+batch_size]\n",
    "            batch_labels = y_train[i:i+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_ts, batch_static)\n",
    "            loss = criterion(outputs.squeeze(), batch_labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Print training progress\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# === Train the Model ===\n",
    "train_model(model, X_train_ts_tensor, X_train_static_tensor, y_train_tensor)\n",
    "\n",
    "# === Evaluate the Model ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_ts_tensor, X_test_static_tensor).squeeze()\n",
    "    y_pred = y_pred.numpy()\n",
    "    \n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    auprc = average_precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"LSTM Model - Test Set C Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.8592\n",
      "Epoch 2/20, Loss: 0.7491\n",
      "Epoch 3/20, Loss: 0.6799\n",
      "Epoch 4/20, Loss: 0.6178\n",
      "Epoch 5/20, Loss: 0.5649\n",
      "Epoch 6/20, Loss: 0.5254\n",
      "Epoch 7/20, Loss: 0.4697\n",
      "Epoch 8/20, Loss: 0.4436\n",
      "Epoch 9/20, Loss: 0.3925\n",
      "Epoch 10/20, Loss: 0.3705\n",
      "Epoch 11/20, Loss: 0.3243\n",
      "Epoch 12/20, Loss: 0.3049\n",
      "Epoch 13/20, Loss: 0.2846\n",
      "Epoch 14/20, Loss: 0.2514\n",
      "Epoch 15/20, Loss: 0.2284\n",
      "Epoch 16/20, Loss: 0.2175\n",
      "Epoch 17/20, Loss: 0.2025\n",
      "Epoch 18/20, Loss: 0.2172\n",
      "Epoch 19/20, Loss: 0.1697\n",
      "Epoch 20/20, Loss: 0.1859\n",
      "Bidirectional LSTM Model - Test Set C Performance: AUROC: 0.7551, AUPRC: 0.3387\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# === Define the Bidirectional LSTM Model in PyTorch ===\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_dim_ts, hidden_dim=64, dropout_rate=0.3):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_dim_ts, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim*2, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)  # Binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass through first bidirectional LSTM\n",
    "        x, _ = self.lstm1(x)  \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through second bidirectional LSTM\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Take the output from the last timestep\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x  # Sigmoid for binary classification\n",
    "\n",
    "# === Data Preparation for PyTorch ===\n",
    "def prepare_lstm_data_for_pytorch(df, time_series_vars, static_vars):\n",
    "    time_series_data, static_data, labels = prepare_lstm_data(df, time_series_vars, static_vars)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    time_series_data = torch.tensor(time_series_data, dtype=torch.float32)\n",
    "    static_data = torch.tensor(static_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    return time_series_data, static_data, labels\n",
    "\n",
    "# Prepare training and test data\n",
    "X_train_ts, X_train_static, y_train = prepare_lstm_data_for_pytorch(df_a, time_series_vars, static_vars)\n",
    "X_test_ts, X_test_static, y_test = prepare_lstm_data_for_pytorch(df_c, time_series_vars, static_vars)\n",
    "\n",
    "# Create datasets and dataloaders for training and testing\n",
    "train_dataset = TensorDataset(X_train_ts, X_train_static, y_train)\n",
    "test_dataset = TensorDataset(X_test_ts, X_test_static, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# === Define the Training Function ===\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            time_series_data, _, labels = batch\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            outputs = model(time_series_data)  # Forward pass\n",
    "            \n",
    "            loss = criterion(outputs.squeeze(), labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# === Train the Model ===\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim_ts = X_train_ts.shape[2]  # Number of time series features\n",
    "model = BidirectionalLSTM(input_dim_ts)\n",
    "\n",
    "# Calculate class weights\n",
    "total_samples = len(df_train)\n",
    "pos_weight = total_samples / (2 * df_train['In_hospital_death'].sum())\n",
    "\n",
    "# Define the loss function with class weights\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]))\n",
    "\n",
    "#criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=20)\n",
    "\n",
    "# === Evaluate the Model ===\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            time_series_data, _, labels = batch\n",
    "            outputs = model(time_series_data)\n",
    "            \n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(outputs.squeeze().numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    auroc = roc_auc_score(y_true, y_pred)\n",
    "    auprc = average_precision_score(y_true, y_pred)\n",
    "    \n",
    "    return auroc, auprc\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "auroc, auprc = evaluate_model(model, test_loader)\n",
    "print(f\"Bidirectional LSTM Model - Test Set C Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.3a Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.4404\n",
      "Epoch 2/20, Loss: 0.3822\n",
      "Epoch 3/20, Loss: 0.3518\n",
      "Epoch 4/20, Loss: 0.3364\n",
      "Epoch 5/20, Loss: 0.3255\n",
      "Epoch 6/20, Loss: 0.3150\n",
      "Epoch 7/20, Loss: 0.3106\n",
      "Epoch 8/20, Loss: 0.3018\n",
      "Epoch 9/20, Loss: 0.2906\n",
      "Epoch 10/20, Loss: 0.2827\n",
      "Epoch 11/20, Loss: 0.2699\n",
      "Epoch 12/20, Loss: 0.2618\n",
      "Epoch 13/20, Loss: 0.2485\n",
      "Epoch 14/20, Loss: 0.2395\n",
      "Epoch 15/20, Loss: 0.2259\n",
      "Epoch 16/20, Loss: 0.2148\n",
      "Epoch 17/20, Loss: 0.2029\n",
      "Epoch 18/20, Loss: 0.1941\n",
      "Epoch 19/20, Loss: 0.1822\n",
      "Epoch 20/20, Loss: 0.1790\n",
      "Transformer Model - Test Set Performance: AUROC: 0.3366, AUPRC: 0.1091\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, time_series_input_size, static_input_size, num_heads=4, hidden_dim=64, projected_dim=40):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Project time-series features to an embedding size divisible by num_heads\n",
    "        self.input_projection = nn.Linear(time_series_input_size, projected_dim)\n",
    "        \n",
    "        # Multi-Head Attention Layer\n",
    "        self.attention_layer = nn.MultiheadAttention(embed_dim=projected_dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # Fully connected layers after attention\n",
    "        self.fc1 = nn.Linear(projected_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Process static features\n",
    "        self.static_fc = nn.Linear(static_input_size, hidden_dim)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, time_series_input, static_input):\n",
    "        # Project time-series features\n",
    "        time_series_input = self.input_projection(time_series_input)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_output, _ = self.attention_layer(time_series_input, time_series_input, time_series_input)\n",
    "        \n",
    "        # Pooling\n",
    "        x = attention_output.mean(dim=1)  # Global average pooling\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        # Process static features\n",
    "        static_out = self.static_fc(static_input)\n",
    "        \n",
    "        # Combine time-series and static features\n",
    "        combined = x + static_out  # Element-wise addition\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        # Final output\n",
    "        output = self.output_fc(combined)\n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "\n",
    "df_train = df_a\n",
    "df_validate = df_b\n",
    "df_test = df_c\n",
    "\n",
    "# Define static and time-series variables\n",
    "static_vars = ['Age', 'Gender', 'Height', 'Weight']\n",
    "time_series_vars = [col for col in df_train.columns if col not in static_vars + ['Hour', 'PatientID', 'RecordID', 'In_hospital_death']]\n",
    "\n",
    "def prepare_data(df):\n",
    "    grouped = df.groupby('PatientID')\n",
    "    \n",
    "    # Time-series tensor: (num_patients, seq_len, num_features)\n",
    "    time_series_data = np.array([group[time_series_vars].values for _, group in grouped])\n",
    "    static_data = grouped[static_vars].last().values  # Static features (num_patients, num_static_features)\n",
    "    labels = grouped['In_hospital_death'].last().values  # Labels\n",
    "    \n",
    "    return time_series_data, static_data, labels\n",
    "\n",
    "# Prepare training & test sets\n",
    "X_train_ts, X_train_static, y_train = prepare_data(df_train)\n",
    "X_test_ts, X_test_static, y_test = prepare_data(df_test)\n",
    "\n",
    "# Standardization\n",
    "scaler_ts = StandardScaler()\n",
    "scaler_static = StandardScaler()\n",
    "\n",
    "X_train_ts = np.array([scaler_ts.fit_transform(seq) for seq in X_train_ts])\n",
    "X_test_ts = np.array([scaler_ts.transform(seq) for seq in X_test_ts])\n",
    "\n",
    "X_train_static = scaler_static.fit_transform(X_train_static)\n",
    "X_test_static = scaler_static.transform(X_test_static)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_ts = torch.tensor(X_train_ts, dtype=torch.float32)\n",
    "X_train_static = torch.tensor(X_train_static, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_ts = torch.tensor(X_test_ts, dtype=torch.float32)\n",
    "X_test_static = torch.tensor(X_test_static, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_ts, X_train_static, y_train)\n",
    "test_dataset = TensorDataset(X_test_ts, X_test_static, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Model Initialization with Adjustments\n",
    "model = TransformerModel(\n",
    "    time_series_input_size=len(time_series_vars),\n",
    "    static_input_size=len(static_vars),\n",
    "    num_heads=16,  # Increased number of attention heads\n",
    "    hidden_dim=256,  # Increased hidden dimensions\n",
    "    projected_dim=128  # Increased projection dimension\n",
    ")\n",
    "\n",
    "# Adjusted Loss Function with Class Weights\n",
    "pos_weight = torch.tensor([len(y_train) / (2 * sum(y_train))], dtype=torch.float32)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# Optimizer with Weight Decay\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training Loop with Validation\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_ts, batch_static, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_ts, batch_static)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_ts, batch_static, batch_y in test_loader:\n",
    "            output = model(batch_ts, batch_static)\n",
    "            loss = criterion(output, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {total_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(test_loader):.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_preds = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for batch_ts, batch_static, batch_y in test_loader:\n",
    "        output = model(batch_ts, batch_static).cpu().numpy()\n",
    "        y_preds.extend(output)\n",
    "        y_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "y_preds = np.array(y_preds).flatten()\n",
    "y_true = np.array(y_true).flatten()\n",
    "\n",
    "auroc = roc_auc_score(y_true, y_preds)\n",
    "auprc = average_precision_score(y_true, y_preds)\n",
    "print(f\"Transformer Model - Test Set Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.3b Tokenizing Time-Series Data and Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 1.          0.          0.         ...  0.          0.\n",
      "  -1.        ]\n",
      " [ 1.          0.          0.         ...  0.          0.\n",
      "  -0.89602804]\n",
      " [ 1.          0.          0.         ...  1.          0.\n",
      "  -2.71591909]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Combine time-series and static variables\n",
    "z_vars = time_series_vars + static_vars\n",
    "\n",
    "# One-hot encode all variables (z)\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "z_encoded = enc.fit_transform(np.array(z_vars).reshape(-1, 1))\n",
    "var_dict = {var: z_encoded[i] for i, var in enumerate(z_vars)}\n",
    "\n",
    "# Normalize time (t) for each dataset\n",
    "df_train['t'] = df_train['Hour'] / df_train['Hour'].max()\n",
    "df_validate['t'] = df_validate['Hour'] / df_train['Hour'].max()\n",
    "df_test['t'] = df_test['Hour'] / df_train['Hour'].max()\n",
    "\n",
    "# Fit scalers for all variables\n",
    "scalers = {var: RobustScaler().fit(df_train[[var]].dropna()) for var in z_vars}\n",
    "\n",
    "# Function to process patient data\n",
    "def process_patient_data(patient_df, scalers):\n",
    "    # Normalize time (t)\n",
    "    t = patient_df['t'].values\n",
    "\n",
    "    # Scale values (v) for all variables\n",
    "    v_scaled = np.column_stack([\n",
    "        scalers[var].transform(patient_df[[var]].fillna(0))[:, 0] for var in z_vars\n",
    "    ])\n",
    "\n",
    "    # Encode variable types (z)\n",
    "    z_encoded = np.array([var_dict[var] for var in z_vars])\n",
    "\n",
    "    # Repeat t for each variable and flatten\n",
    "    t_repeated = np.repeat(t, len(z_vars)).reshape(-1, 1)\n",
    "\n",
    "    # Tile z_encoded for each time step\n",
    "    z_tiled = np.tile(z_encoded, (len(t), 1))\n",
    "\n",
    "    # Flatten v_scaled\n",
    "    v_flattened = v_scaled.flatten().reshape(-1, 1)\n",
    "\n",
    "    # Combine into triplets (t, z, v) as a single NumPy array\n",
    "    triplets = np.hstack((t_repeated, z_tiled, v_flattened))\n",
    "\n",
    "    return triplets\n",
    "\n",
    "# Process datasets\n",
    "patient_groups_train = df_train.groupby('PatientID')\n",
    "patient_groups_validate = df_validate.groupby('PatientID')\n",
    "patient_groups_test = df_test.groupby('PatientID')\n",
    "\n",
    "all_patients_train = [process_patient_data(group, scalers) for _, group in patient_groups_train]\n",
    "all_patients_validate = [process_patient_data(group, scalers) for _, group in patient_groups_validate]\n",
    "all_patients_test = [process_patient_data(group, scalers) for _, group in patient_groups_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.9093\n",
      "Epoch 2/2, Loss: 0.8965\n",
      "Transformer Model - Test Set Performance: AUROC: 0.4974, AUPRC: 0.1457\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_ts = [torch.tensor([x[0] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_train]\n",
    "X_train_z = [torch.tensor([x[1] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_train]\n",
    "X_train_v = [torch.tensor([x[2] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_train]\n",
    "\n",
    "X_validate_ts = [torch.tensor([x[0] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_validate]\n",
    "X_validate_z = [torch.tensor([x[1] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_validate]\n",
    "X_validate_v = [torch.tensor([x[2] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_validate]\n",
    "\n",
    "X_test_ts = [torch.tensor([x[0] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_test]\n",
    "X_test_z = [torch.tensor([x[1] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_test]\n",
    "X_test_v = [torch.tensor([x[2] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_test]\n",
    "\n",
    "# Dataset preparation (combining t, z, v into a single tensor)\n",
    "train_data = []\n",
    "for t, z, v, label in zip(X_train_ts, X_train_z, X_train_v, y_train):\n",
    "    # Ensure z has the correct shape\n",
    "    if z.dim() == 2 and z.shape[1] == 1:  # If z is incorrectly shaped, fix it\n",
    "        z = z.squeeze(1)  # Remove the extra dimension\n",
    "        z = torch.nn.functional.one_hot(z.to(torch.int64), num_classes=len(var_dict)).float()\n",
    "    combined = torch.cat((t, z, v), dim=1)  # Concatenate along the feature axis\n",
    "    train_data.append((combined, torch.tensor(label,dtype=torch.float32)))  # Replace 0.0 with the actual label if available\n",
    "\n",
    "validate_data = []\n",
    "for t, z, v, label in zip(X_validate_ts, X_validate_z, X_validate_v, y_validate):\n",
    "    if z.dim() == 2 and z.shape[1] == 1:\n",
    "        z = z.squeeze(1)\n",
    "        z = torch.nn.functional.one_hot(z.to(torch.int64), num_classes=len(var_dict)).float()\n",
    "    combined = torch.cat((t, z, v), dim=1)\n",
    "    validate_data.append((combined, torch.tensor(label, dtype=torch.float32)))\n",
    "\n",
    "test_data = []\n",
    "for t, z, v,label in zip(X_test_ts, X_test_z, X_test_v, y_test):\n",
    "    if z.dim() == 2 and z.shape[1] == 1:\n",
    "        z = z.squeeze(1)\n",
    "        z = torch.nn.functional.one_hot(z.to(torch.int64), num_classes=len(var_dict)).float()\n",
    "    combined = torch.cat((t, z, v), dim=1)\n",
    "    test_data.append((combined, torch.tensor(label, dtype=torch.float32)))\n",
    "    \n",
    "# Convert to DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "validate_loader = DataLoader(validate_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, time_series_input_size, num_heads=4, hidden_dim=64, projected_dim=40):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Project time-series features to an embedding size divisible by num_heads\n",
    "        self.input_projection = nn.Linear(time_series_input_size, projected_dim)\n",
    "        \n",
    "        # Multi-Head Attention Layer\n",
    "        self.attention_layer = nn.MultiheadAttention(embed_dim=projected_dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # Fully connected layers after attention\n",
    "        self.fc1 = nn.Linear(projected_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, time_series_input):\n",
    "        # Project input to the correct embedding size\n",
    "        time_series_input = self.input_projection(time_series_input)\n",
    "        # Apply attention\n",
    "        attention_output, _ = self.attention_layer(time_series_input, time_series_input, time_series_input)\n",
    "        \n",
    "        # Pooling\n",
    "        x = attention_output.mean(dim=1)  # Global average pooling\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        # Final output\n",
    "        output = self.output_fc(x)\n",
    "        return self.output_fc(x)  # No sigmoid here\n",
    "\n",
    "# ===== Model Initialization and Training =====\n",
    "model = TransformerModel(time_series_input_size=43,\n",
    "                        num_heads=4,  # Increased number of attention heads\n",
    "                        hidden_dim=256,  # Increased hidden dimensions\n",
    "                        projected_dim=128  # Increased projection dimension)\n",
    "                    )\n",
    "# Use BCEWithLogitsLoss with class weights for imbalance\n",
    "pos_weight = torch.tensor([len(y_train) / (2 * sum(y_train))], dtype=torch.float32)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "# Learning Rate Scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "epochs = 5\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_ts, batch_y in train_loader:  # Adjust unpacking to match the dataset\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_ts)  # Pass only the time-series input to the model\n",
    "        batch_y = batch_y.view(-1, 1)  # Reshape labels to match the output shape\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    scheduler.step()\n",
    "    \n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_preds = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        output = model(batch_x).cpu().numpy()\n",
    "        y_preds.extend(output)\n",
    "        y_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "y_preds = np.array(y_preds).flatten()\n",
    "y_true = np.array(y_true).flatten()\n",
    "\n",
    "auroc = roc_auc_score(y_true, y_preds)\n",
    "auprc = average_precision_score(y_true, y_preds)\n",
    "print(f\"Transformer Model - Test Set Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

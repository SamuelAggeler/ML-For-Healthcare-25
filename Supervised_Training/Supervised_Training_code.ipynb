{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1 Classic Machine Learning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tsfresh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVC \n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, average_precision_score\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtsfresh\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_features, MinimalFCParameters\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtsfresh\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_features\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Ensure tsfresh is installed\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tsfresh'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tsfresh.feature_extraction import extract_features, MinimalFCParameters\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import make_forecasting_frame\n",
    "from tsfresh.feature_extraction.settings import MinimalFCParameters\n",
    "#from tensorflow.keras.models import Sequential, Model\n",
    "#from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional,MultiHeadAttention, Flatten, Input, Embedding\n",
    "#from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - AUROC: 0.8579, AUPRC: 0.5355\n",
      "Random Forest - AUROC: 0.8464, AUPRC: 0.5210\n",
      "SVM - AUROC: 0.8445, AUPRC: 0.5029\n"
     ]
    }
   ],
   "source": [
    "# === PART 1 OF Q2.1 ===\n",
    "# Load preprocessed datasets\n",
    "df_a = pd.read_parquet('ml_ready_data/set-a-scaled.parquet')\n",
    "df_b = pd.read_parquet('ml_ready_data/set-b-scaled.parquet')\n",
    "df_c = pd.read_parquet('ml_ready_data/set-c-scaled.parquet')  # Test set\n",
    "\n",
    "# Combine training sets\n",
    "df_train = pd.concat([df_a, df_b], ignore_index=True)\n",
    "df_test = df_c  # Test set is set-c\n",
    "\n",
    "# Define static and dynamic variables\n",
    "static_vars = ['Age', 'Gender', 'Height', 'Weight']\n",
    "time_series_vars = [col for col in df_train.columns if col not in static_vars + ['Hour', 'PatientID', 'RecordID', 'In_hospital_death']]\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(df):\n",
    "    features = df.groupby('PatientID')[time_series_vars].agg(['mean', 'max', 'last'])\n",
    "    features.columns = ['_'.join(col) for col in features.columns]\n",
    "    \n",
    "    # Add static variables\n",
    "    static_data = df.groupby('PatientID')[static_vars].last()\n",
    "    features = features.merge(static_data, left_index=True, right_index=True)\n",
    "    \n",
    "    # Add labels\n",
    "    labels = df.groupby('PatientID')['In_hospital_death'].last()\n",
    "    features['In_hospital_death'] = labels\n",
    "    return features.reset_index()\n",
    "\n",
    "# Extract features\n",
    "df_train_features = extract_features(df_train)\n",
    "df_test_features = extract_features(df_test)\n",
    "\n",
    "# Prepare data for ML\n",
    "X_train = df_train_features.drop(columns=['PatientID', 'In_hospital_death'])\n",
    "y_train = df_train_features['In_hospital_death']\n",
    "X_test = df_test_features.drop(columns=['PatientID', 'In_hospital_death'])\n",
    "y_test = df_test_features['In_hospital_death']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train classifiers\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(model, X_test, y_test, name):\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    auprc = average_precision_score(y_test, y_pred)\n",
    "    print(f\"{name} - AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "# Report performance\n",
    "evaluate_model(logreg, X_test, y_test, 'Logistic Regression')\n",
    "evaluate_model(rf, X_test, y_test, 'Random Forest')\n",
    "evaluate_model(svm_model, X_test, y_test, 'SVM')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-51:0/50 [41:59<?, ?it/s]\n",
      "Process ForkPoolWorker-58:\n",
      "Process ForkPoolWorker-56:\n",
      "Process ForkPoolWorker-60:\n",
      "Process ForkPoolWorker-52:\n",
      "Process ForkPoolWorker-54:\n",
      "Process ForkPoolWorker-57:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-59:\n",
      "Process ForkPoolWorker-55:\n",
      "Process ForkPoolWorker-53:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 387, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/pool.py:856\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m     item = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_items\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[31mIndexError\u001b[39m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ts_features.reset_index()\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Extract features with tsfresh\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m df_train_features = \u001b[43mextract_features_with_tsfresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m df_test_features = extract_features_with_tsfresh(df_test)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Prepare data for ML\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mextract_features_with_tsfresh\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     17\u001b[39m reshaped_data = []\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m time_series_vars:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     temp_df, _ = \u001b[43mmake_forecasting_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_timeshift\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m48\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrolling_direction\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     reshaped_data.append(temp_df)\n\u001b[32m     22\u001b[39m reshaped_df = pd.concat(reshaped_data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/tsfresh/utilities/dataframe_functions.py:619\u001b[39m, in \u001b[36mmake_forecasting_frame\u001b[39m\u001b[34m(x, kind, max_timeshift, rolling_direction)\u001b[39m\n\u001b[32m    615\u001b[39m     t = \u001b[38;5;28mrange\u001b[39m(n)\n\u001b[32m    617\u001b[39m df = pd.DataFrame({\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m] * n, \u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m: t, \u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m: x, \u001b[33m\"\u001b[39m\u001b[33mkind\u001b[39m\u001b[33m\"\u001b[39m: kind})\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m df_shift = \u001b[43mroll_time_series\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_kind\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkind\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrolling_direction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrolling_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_timeshift\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_timeshift\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[38;5;66;03m# drop the rows which should actually be predicted\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmask_first\u001b[39m(x):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/tsfresh/utilities/dataframe_functions.py:568\u001b[39m, in \u001b[36mroll_time_series\u001b[39m\u001b[34m(df_or_dict, column_id, column_sort, column_kind, rolling_direction, max_timeshift, min_timeshift, chunksize, n_jobs, show_warnings, disable_progressbar, distributor)\u001b[39m\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mthe passed distributor is not an DistributorBaseClass object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    559\u001b[39m kwargs = {\n\u001b[32m    560\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgrouped_data\u001b[39m\u001b[33m\"\u001b[39m: grouped_data,\n\u001b[32m    561\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrolling_direction\u001b[39m\u001b[33m\"\u001b[39m: rolling_direction,\n\u001b[32m   (...)\u001b[39m\u001b[32m    565\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumn_id\u001b[39m\u001b[33m\"\u001b[39m: column_id,\n\u001b[32m    566\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m shifted_chunks = \u001b[43mdistributor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_roll_out_time_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrange_of_shifts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m distributor.close()\n\u001b[32m    577\u001b[39m df_shift = pd.concat(shifted_chunks, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/tsfresh/utilities/distribution.py:241\u001b[39m, in \u001b[36mIterableDistributorBaseClass.map_reduce\u001b[39m\u001b[34m(self, map_function, data, function_kwargs, chunk_size, data_length)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    235\u001b[39m     result = (\n\u001b[32m    236\u001b[39m         \u001b[38;5;28mself\u001b[39m.distribute(\n\u001b[32m    237\u001b[39m             _function_with_partly_reduce, chunk_generator, map_kwargs\n\u001b[32m    238\u001b[39m         ),\n\u001b[32m    239\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m result = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/pool.py:861\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28mself\u001b[39m._pool = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    863\u001b[39m     item = \u001b[38;5;28mself\u001b[39m._items.popleft()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === PART 2 OF Q2.1 ===\n",
    "\n",
    "# Extract features with tsfresh\n",
    "\n",
    "# Extract features with tsfresh\n",
    "def extract_tsfresh_features(df):\n",
    "    df_filtered = df[['PatientID', 'Hour'] + time_series_vars].copy()\n",
    "    df_filtered = df_filtered.melt(id_vars=['PatientID', 'Hour'], var_name='variable', value_name='value')\n",
    "    df_filtered = df_filtered.dropna()\n",
    "    df_filtered['id'] = df_filtered['PatientID']\n",
    "    df_filtered['time'] = df_filtered['Hour']\n",
    "    \n",
    "    ts_features = extract_features(\n",
    "        df_filtered, \n",
    "        column_id='id', \n",
    "        column_sort='time', \n",
    "        column_value='value', \n",
    "        column_kind='variable', \n",
    "        default_fc_parameters=MinimalFCParameters(),\n",
    "        n_jobs=1\n",
    "    )\n",
    "    \n",
    "    # Add static variables\n",
    "    static_data = df.groupby('PatientID')[static_vars].last()\n",
    "    ts_features = ts_features.merge(static_data, left_index=True, right_index=True)\n",
    "    \n",
    "    # Add labels\n",
    "    labels = df.groupby('PatientID')['In_hospital_death'].last()\n",
    "    ts_features['In_hospital_death'] = labels\n",
    "    \n",
    "    return ts_features.reset_index()\n",
    "\n",
    "# Extract features\n",
    "df_train_features = extract_tsfresh_features(df_train)\n",
    "df_test_features = extract_tsfresh_features(df_test)\n",
    "\n",
    "# Prepare data for ML\n",
    "X_train = df_train_features.drop(columns=['PatientID', 'In_hospital_death'])\n",
    "y_train = df_train_features['In_hospital_death']\n",
    "X_test = df_test_features.drop(columns=['PatientID', 'In_hospital_death'])\n",
    "y_test = df_test_features['In_hospital_death']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train classifiers\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(model, X_test, y_test, name):\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    auprc = average_precision_score(y_test, y_pred)\n",
    "    print(f\"{name} - AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "# Report performance\n",
    "evaluate_model(logreg, X_test, y_test, 'Logistic Regression')\n",
    "evaluate_model(rf, X_test, y_test, 'Random Forest')\n",
    "evaluate_model(svm_model, X_test, y_test, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.2 Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Implementing LSTM ===\n",
    "\n",
    "# Prepare data for LSTM\n",
    "def prepare_lstm_data(df, time_series_vars, static_vars):\n",
    "    # Group by PatientID and create 3D arrays for time-series data\n",
    "    time_series_data = df.groupby('PatientID')[time_series_vars].apply(lambda x: x.values)\n",
    "    static_data = df.groupby('PatientID')[static_vars].last()\n",
    "    labels = df.groupby('PatientID')['In_hospital_death'].last()\n",
    "    \n",
    "    # Pad sequences to ensure uniform length\n",
    "    max_timesteps = max(time_series_data.apply(len))\n",
    "    time_series_data = np.array([np.pad(x, ((0, max_timesteps - len(x)), (0, 0)), mode='constant') for x in time_series_data])\n",
    "    \n",
    "    return time_series_data, static_data.values, labels.values\n",
    "\n",
    "# Prepare training and test data\n",
    "X_train_ts, X_train_static, y_train = prepare_lstm_data(df_a, time_series_vars, static_vars)\n",
    "X_test_ts, X_test_static, y_test = prepare_lstm_data(df_c, time_series_vars, static_vars)\n",
    "\n",
    "# Standardize static and time-series features\n",
    "scaler_static = StandardScaler()\n",
    "X_train_static = scaler_static.fit_transform(X_train_static)\n",
    "X_test_static = scaler_static.transform(X_test_static)\n",
    "\n",
    "scaler_ts = StandardScaler()\n",
    "X_train_ts = np.array([scaler_ts.fit_transform(x) for x in X_train_ts])\n",
    "X_test_ts = np.array([scaler_ts.transform(x) for x in X_test_ts])\n",
    "\n",
    "# Build LSTM model\n",
    "def build_lstm_model(input_shape_ts, input_shape_static):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=input_shape_ts, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(32, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define input shapes\n",
    "input_shape_ts = (X_train_ts.shape[1], X_train_ts.shape[2])  # (timesteps, features)\n",
    "input_shape_static = X_train_static.shape[1]  # Number of static features\n",
    "\n",
    "# Build and train the model\n",
    "lstm_model = build_lstm_model(input_shape_ts, input_shape_static)\n",
    "history = lstm_model.fit(X_train_ts, y_train, validation_split=0.2, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = lstm_model.predict(X_test_ts).flatten()\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "auprc = average_precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"LSTM Model - Test Set C Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build Bidirectional LSTM model ===\n",
    "\n",
    "def build_bidirectional_lstm_model(input_shape_ts):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape_ts))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define input shape for time-series data\n",
    "input_shape_ts = (X_train_ts.shape[1], X_train_ts.shape[2])  # (timesteps, features)\n",
    "\n",
    "# Build and train the bidirectional LSTM model\n",
    "bidirectional_lstm_model = build_bidirectional_lstm_model(input_shape_ts)\n",
    "history = bidirectional_lstm_model.fit(X_train_ts, y_train, validation_split=0.2, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = bidirectional_lstm_model.predict(X_test_ts).flatten()\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "auprc = average_precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Bidirectional LSTM Model - Test Set C Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.3a Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Transformer model\n",
    "def build_transformer_model(input_shape_ts):\n",
    "    inputs = Input(shape=input_shape_ts)  # Input shape: (timesteps, features)\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    attention_output = MultiHeadAttention(num_heads=4, key_dim=input_shape_ts[1])(inputs, inputs)\n",
    "    attention_output = Dropout(0.3)(attention_output)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output + inputs)\n",
    "    \n",
    "    # Feed-Forward Network\n",
    "    ff_output = Dense(64, activation='relu')(attention_output)\n",
    "    ff_output = Dropout(0.3)(ff_output)\n",
    "    ff_output = Dense(32, activation='relu')(ff_output)\n",
    "    ff_output = LayerNormalization(epsilon=1e-6)(ff_output + attention_output)\n",
    "    \n",
    "    # Flatten and Output Layer\n",
    "    flatten_output = Flatten()(ff_output)\n",
    "    outputs = Dense(1, activation='sigmoid')(flatten_output)  # Binary classification\n",
    "    \n",
    "    # Compile Model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define input shape for time-series data\n",
    "input_shape_ts = (X_train_ts.shape[1], X_train_ts.shape[2])  # (timesteps, features)\n",
    "\n",
    "# Build and train the transformer model\n",
    "transformer_model = build_transformer_model(input_shape_ts)\n",
    "history = transformer_model.fit(X_train_ts, y_train, validation_split=0.2, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = transformer_model.predict(X_test_ts).flatten()\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "auprc = average_precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Transformer Model - Test Set C Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.3b Tokenizing Time-Series Data and Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "input_path = \"processed_data\"\n",
    "output_path = \"tokenized_data\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Define static and time-series variables\n",
    "static_vars = ['Age', 'Gender', 'Height', 'Weight']\n",
    "time_series_vars = []  # Will be populated dynamically\n",
    "\n",
    "# Load one dataset to infer time-series variable names\n",
    "df_sample = pd.read_parquet(os.path.join(input_path, \"set-a.parquet\"))\n",
    "time_series_vars = [col for col in df_sample.columns if col not in static_vars + ['Hour', 'PatientID', 'RecordID', 'In_hospital_death']]\n",
    "\n",
    "# Normalize time (t) to range [0,1]\n",
    "def scale_time(hour):\n",
    "    return hour / 48.0  # Since we have 48-hour windows\n",
    "\n",
    "# Encode variable names (z) as categorical integers\n",
    "variable_encoder = LabelEncoder()\n",
    "variable_encoder.fit(time_series_vars)\n",
    "\n",
    "# Scale observed values (v)\n",
    "value_scalers = {var: MinMaxScaler() for var in time_series_vars}\n",
    "\n",
    "# Fit scalers using set A (training set)\n",
    "df_train = pd.read_parquet(os.path.join(input_path, \"set-a.parquet\"))\n",
    "for var in time_series_vars:\n",
    "    df_train[var] = df_train[var].fillna(0)  # Fill missing values before scaling\n",
    "    value_scalers[var].fit(df_train[[var]])\n",
    "\n",
    "def tokenize_patient_data(df):\n",
    "    tokenized_data = []\n",
    "    \n",
    "    for _, patient_group in df.groupby(\"PatientID\"):\n",
    "        patient_id = patient_group[\"PatientID\"].iloc[0]\n",
    "        \n",
    "        for _, row in patient_group.iterrows():\n",
    "            for var in time_series_vars:\n",
    "                if pd.notna(row[var]):  # Only use observed values\n",
    "                    t = scale_time(row['Hour'])\n",
    "                    z = variable_encoder.transform([var])[0]  # Encode variable\n",
    "                    v = value_scalers[var].transform([[row[var]]])[0][0]  # Scale value\n",
    "                    tokenized_data.append([patient_id, t, z, v])\n",
    "    \n",
    "    return pd.DataFrame(tokenized_data, columns=['PatientID', 't', 'z', 'v'])\n",
    "\n",
    "# Process datasets\n",
    "for set_id in ['a', 'b', 'c']:\n",
    "    print(f\"Tokenizing set {set_id}...\")\n",
    "    df = pd.read_parquet(os.path.join(input_path, f\"set-{set_id}.parquet\"))\n",
    "    tokenized_df = tokenize_patient_data(df)\n",
    "    tokenized_df.to_parquet(os.path.join(output_path, f\"tokenized-set-{set_id}.parquet\"))\n",
    "    print(f\"Saved tokenized set {set_id} with {len(tokenized_df)} rows.\")\n",
    "\n",
    "print(\"Tokenization complete. Data saved to tokenized_data/\")\n",
    "\n",
    "# Load tokenized datasets\n",
    "def load_tokenized_data(set_id):\n",
    "    return pd.read_parquet(f\"tokenized_data/tokenized-set-{set_id}.parquet\")\n",
    "\n",
    "train_df = load_tokenized_data('a')\n",
    "test_df = load_tokenized_data('c')  # Test set\n",
    "\n",
    "# Prepare input tensors\n",
    "max_seq_len = 500  # Maximum sequence length\n",
    "num_variables = 41  # Number of unique variables (z values)\n",
    "\n",
    "# Pad sequences to max length\n",
    "def prepare_sequences(df, max_seq_len):\n",
    "    grouped = df.groupby(\"PatientID\").apply(lambda x: x.sort_values(\"t\").iloc[:max_seq_len])\n",
    "    X_t = grouped[\"t\"].groupby(\"PatientID\").apply(lambda x: np.pad(x.values, (0, max_seq_len - len(x)), 'constant'))\n",
    "    X_z = grouped[\"z\"].groupby(\"PatientID\").apply(lambda x: np.pad(x.values, (0, max_seq_len - len(x)), 'constant'))\n",
    "    X_v = grouped[\"v\"].groupby(\"PatientID\").apply(lambda x: np.pad(x.values, (0, max_seq_len - len(x)), 'constant'))\n",
    "    y = grouped[\"PatientID\"].apply(lambda x: x[\"v\"].iloc[0])  # Dummy target (modify if needed)\n",
    "    return np.stack([X_t, X_z, X_v], axis=-1), y.values\n",
    "\n",
    "X_train, y_train = prepare_sequences(train_df, max_seq_len)\n",
    "X_test, y_test = prepare_sequences(test_df, max_seq_len)\n",
    "\n",
    "# Transformer model definition\n",
    "def build_transformer_model(input_shape, num_variables, d_model=64, num_heads=4, ff_dim=128):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Variable Embedding\n",
    "    time_input = inputs[:, :, 0:1]\n",
    "    variable_input = Embedding(input_dim=num_variables, output_dim=d_model)(inputs[:, :, 1])\n",
    "    value_input = Dense(d_model)(inputs[:, :, 2:3])\n",
    "    \n",
    "    # Combine embeddings\n",
    "    x = tf.keras.layers.Concatenate()([time_input, variable_input, value_input])\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "    attn_output = Dropout(0.1)(attn_output)\n",
    "    attn_output = LayerNormalization(epsilon=1e-6)(attn_output + x)\n",
    "    \n",
    "    # Feed-Forward Layer\n",
    "    ff_output = Dense(ff_dim, activation='relu')(attn_output)\n",
    "    ff_output = Dense(d_model)(ff_output)\n",
    "    ff_output = Dropout(0.1)(ff_output)\n",
    "    ff_output = LayerNormalization(epsilon=1e-6)(ff_output + attn_output)\n",
    "    \n",
    "    # Classification Head\n",
    "    x = Flatten()(ff_output)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "# Build and train model\n",
    "model = build_transformer_model((max_seq_len, 3), num_variables)\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_probs = model.predict(X_test).flatten()\n",
    "auroc = roc_auc_score(y_test, y_pred_probs)\n",
    "auprc = average_precision_score(y_test, y_pred_probs)\n",
    "\n",
    "print(f\"Test AUROC: {auroc:.4f}\")\n",
    "print(f\"Test AUPRC: {auprc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

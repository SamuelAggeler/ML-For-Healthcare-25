{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1 Classic Machine Learning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tsfresh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVC \n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, average_precision_score\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtsfresh\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_features, EfficientFCParameters\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Dropout\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tsfresh'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tsfresh.feature_extraction import extract_features, EfficientFCParameters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional,MultiHeadAttention, Flatten, Input, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PART 1 OF Q2.1 ===\n",
    "# Load preprocessed datasets\n",
    "df_a = pd.read_parquet('processed_data/set-a.parquet')\n",
    "df_b = pd.read_parquet('processed_data/set-b.parquet')\n",
    "df_c = pd.read_parquet('processed_data/set-c.parquet')  # Test set\n",
    "\n",
    "# Combine training sets\n",
    "df_train = pd.concat([df_a, df_b], ignore_index=True)\n",
    "df_test = df_c  # Test set is set-c\n",
    "\n",
    "# Define static and dynamic variables\n",
    "static_vars = ['Age', 'Gender', 'Height', 'Weight']\n",
    "time_series_vars = [col for col in df_train.columns if col not in static_vars + ['Hour', 'PatientID', 'RecordID', 'ICUType', 'In_hospital_death']]\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(df):\n",
    "    features = df.groupby('PatientID')[time_series_vars].agg(['mean', 'max', 'last'])\n",
    "    features.columns = ['_'.join(col) for col in features.columns]\n",
    "    \n",
    "    # Add static variables\n",
    "    static_data = df.groupby('PatientID')[static_vars].last()\n",
    "    features = features.merge(static_data, left_index=True, right_index=True)\n",
    "    \n",
    "    # Add labels\n",
    "    labels = df.groupby('PatientID')['In_hospital_death'].last()\n",
    "    features['In_hospital_death'] = labels\n",
    "    return features.reset_index()\n",
    "\n",
    "# Extract features\n",
    "df_train_features = extract_features(df_train)\n",
    "df_test_features = extract_features(df_test)\n",
    "\n",
    "# Prepare data for ML\n",
    "X_train = df_train_features.drop(columns=['PatientID', 'In_hospital_death'])\n",
    "y_train = df_train_features['In_hospital_death']\n",
    "X_test = df_test_features.drop(columns=['PatientID', 'In_hospital_death'])\n",
    "y_test = df_test_features['In_hospital_death']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train classifiers\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(model, X_test, y_test, name):\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    auprc = average_precision_score(y_test, y_pred)\n",
    "    print(f\"{name} - AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "# Report performance\n",
    "evaluate_model(logreg, X_test, y_test, 'Logistic Regression')\n",
    "evaluate_model(rf, X_test, y_test, 'Random Forest')\n",
    "evaluate_model(svm_model, X_test, y_test, 'SVM')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PART 2 OF Q2.1 ===\n",
    "\n",
    "# Define static and dynamic variables and additional features\n",
    "static_vars = ['Age', 'Gender', 'Height', 'Weight']\n",
    "time_series_vars = [col for col in df_train.columns if col not in static_vars + ['Hour', 'PatientID', 'RecordID', 'ICUType', 'In_hospital_death']]\n",
    "additional_vars = ['Creatinine', 'HR', 'PaCO2', 'PaO2', 'pH','Urine', 'Temp']\n",
    "\n",
    "# Include additional_vars in time_series_vars\n",
    "time_series_vars += additional_vars\n",
    "\n",
    "# Updated feature extraction function\n",
    "def extract_features_with_tsfresh(df):\n",
    "    # Extract time-series features using tsfresh\n",
    "    ts_features = extract_features(\n",
    "        df[['PatientID', 'Hour'] + time_series_vars],\n",
    "        column_id='PatientID',\n",
    "        column_sort='Hour',\n",
    "        default_fc_parameters=EfficientFCParameters(),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Add static variables\n",
    "    static_data = df.groupby('PatientID')[static_vars].last()\n",
    "    ts_features = ts_features.merge(static_data, left_index=True, right_index=True)\n",
    "    \n",
    "    # Add labels\n",
    "    labels = df.groupby('PatientID')['In_hospital_death'].last()\n",
    "    ts_features['In_hospital_death'] = labels\n",
    "    return ts_features.reset_index()\n",
    "\n",
    "# Extract features with tsfresh\n",
    "df_train_features = extract_features_with_tsfresh(df_train)\n",
    "df_test_features = extract_features_with_tsfresh(df_test)\n",
    "\n",
    "# Extract features\n",
    "df_train_features = extract_features(df_train)\n",
    "df_test_features = extract_features(df_test)\n",
    "\n",
    "# Prepare data for ML\n",
    "X_train = df_train_features.drop(columns=['PatientID', 'In_hospital_death'])\n",
    "y_train = df_train_features['In_hospital_death']\n",
    "X_test = df_test_features.drop(columns=['PatientID', 'In_hospital_death'])\n",
    "y_test = df_test_features['In_hospital_death']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train classifiers\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(model, X_test, y_test, name):\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    auprc = average_precision_score(y_test, y_pred)\n",
    "    print(f\"{name} - AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "# Report performance\n",
    "evaluate_model(logreg, X_test, y_test, 'Logistic Regression')\n",
    "evaluate_model(rf, X_test, y_test, 'Random Forest')\n",
    "evaluate_model(svm_model, X_test, y_test, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.2 Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Implementing LSTM ===\n",
    "\n",
    "# Prepare data for LSTM\n",
    "def prepare_lstm_data(df, time_series_vars, static_vars):\n",
    "    # Group by PatientID and create 3D arrays for time-series data\n",
    "    time_series_data = df.groupby('PatientID')[time_series_vars].apply(lambda x: x.values)\n",
    "    static_data = df.groupby('PatientID')[static_vars].last()\n",
    "    labels = df.groupby('PatientID')['In_hospital_death'].last()\n",
    "    \n",
    "    # Pad sequences to ensure uniform length\n",
    "    max_timesteps = max(time_series_data.apply(len))\n",
    "    time_series_data = np.array([np.pad(x, ((0, max_timesteps - len(x)), (0, 0)), mode='constant') for x in time_series_data])\n",
    "    \n",
    "    return time_series_data, static_data.values, labels.values\n",
    "\n",
    "# Prepare training and test data\n",
    "X_train_ts, X_train_static, y_train = prepare_lstm_data(df_a, time_series_vars, static_vars)\n",
    "X_test_ts, X_test_static, y_test = prepare_lstm_data(df_c, time_series_vars, static_vars)\n",
    "\n",
    "# Standardize static and time-series features\n",
    "scaler_static = StandardScaler()\n",
    "X_train_static = scaler_static.fit_transform(X_train_static)\n",
    "X_test_static = scaler_static.transform(X_test_static)\n",
    "\n",
    "scaler_ts = StandardScaler()\n",
    "X_train_ts = np.array([scaler_ts.fit_transform(x) for x in X_train_ts])\n",
    "X_test_ts = np.array([scaler_ts.transform(x) for x in X_test_ts])\n",
    "\n",
    "# Build LSTM model\n",
    "def build_lstm_model(input_shape_ts, input_shape_static):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=input_shape_ts, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(32, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define input shapes\n",
    "input_shape_ts = (X_train_ts.shape[1], X_train_ts.shape[2])  # (timesteps, features)\n",
    "input_shape_static = X_train_static.shape[1]  # Number of static features\n",
    "\n",
    "# Build and train the model\n",
    "lstm_model = build_lstm_model(input_shape_ts, input_shape_static)\n",
    "history = lstm_model.fit(X_train_ts, y_train, validation_split=0.2, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = lstm_model.predict(X_test_ts).flatten()\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "auprc = average_precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"LSTM Model - Test Set C Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build Bidirectional LSTM model ===\n",
    "\n",
    "def build_bidirectional_lstm_model(input_shape_ts):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape_ts))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define input shape for time-series data\n",
    "input_shape_ts = (X_train_ts.shape[1], X_train_ts.shape[2])  # (timesteps, features)\n",
    "\n",
    "# Build and train the bidirectional LSTM model\n",
    "bidirectional_lstm_model = build_bidirectional_lstm_model(input_shape_ts)\n",
    "history = bidirectional_lstm_model.fit(X_train_ts, y_train, validation_split=0.2, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = bidirectional_lstm_model.predict(X_test_ts).flatten()\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "auprc = average_precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Bidirectional LSTM Model - Test Set C Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.3a Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Transformer model\n",
    "def build_transformer_model(input_shape_ts):\n",
    "    inputs = Input(shape=input_shape_ts)  # Input shape: (timesteps, features)\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    attention_output = MultiHeadAttention(num_heads=4, key_dim=input_shape_ts[1])(inputs, inputs)\n",
    "    attention_output = Dropout(0.3)(attention_output)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output + inputs)\n",
    "    \n",
    "    # Feed-Forward Network\n",
    "    ff_output = Dense(64, activation='relu')(attention_output)\n",
    "    ff_output = Dropout(0.3)(ff_output)\n",
    "    ff_output = Dense(32, activation='relu')(ff_output)\n",
    "    ff_output = LayerNormalization(epsilon=1e-6)(ff_output + attention_output)\n",
    "    \n",
    "    # Flatten and Output Layer\n",
    "    flatten_output = Flatten()(ff_output)\n",
    "    outputs = Dense(1, activation='sigmoid')(flatten_output)  # Binary classification\n",
    "    \n",
    "    # Compile Model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define input shape for time-series data\n",
    "input_shape_ts = (X_train_ts.shape[1], X_train_ts.shape[2])  # (timesteps, features)\n",
    "\n",
    "# Build and train the transformer model\n",
    "transformer_model = build_transformer_model(input_shape_ts)\n",
    "history = transformer_model.fit(X_train_ts, y_train, validation_split=0.2, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = transformer_model.predict(X_test_ts).flatten()\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "auprc = average_precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Transformer Model - Test Set C Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.3b Tokenizing Time-Series Data and Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "input_path = \"processed_data\"\n",
    "output_path = \"tokenized_data\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Define static and time-series variables\n",
    "static_vars = ['Age', 'Gender', 'Height', 'Weight']\n",
    "time_series_vars = []  # Will be populated dynamically\n",
    "\n",
    "# Load one dataset to infer time-series variable names\n",
    "df_sample = pd.read_parquet(os.path.join(input_path, \"set-a.parquet\"))\n",
    "time_series_vars = [col for col in df_sample.columns if col not in static_vars + ['Hour', 'PatientID', 'RecordID', 'ICUType', 'In_hospital_death']]\n",
    "\n",
    "# Normalize time (t) to range [0,1]\n",
    "def scale_time(hour):\n",
    "    return hour / 48.0  # Since we have 48-hour windows\n",
    "\n",
    "# Encode variable names (z) as categorical integers\n",
    "variable_encoder = LabelEncoder()\n",
    "variable_encoder.fit(time_series_vars)\n",
    "\n",
    "# Scale observed values (v)\n",
    "value_scalers = {var: MinMaxScaler() for var in time_series_vars}\n",
    "\n",
    "# Fit scalers using set A (training set)\n",
    "df_train = pd.read_parquet(os.path.join(input_path, \"set-a.parquet\"))\n",
    "for var in time_series_vars:\n",
    "    df_train[var] = df_train[var].fillna(0)  # Fill missing values before scaling\n",
    "    value_scalers[var].fit(df_train[[var]])\n",
    "\n",
    "def tokenize_patient_data(df):\n",
    "    tokenized_data = []\n",
    "    \n",
    "    for _, patient_group in df.groupby(\"PatientID\"):\n",
    "        patient_id = patient_group[\"PatientID\"].iloc[0]\n",
    "        \n",
    "        for _, row in patient_group.iterrows():\n",
    "            for var in time_series_vars:\n",
    "                if pd.notna(row[var]):  # Only use observed values\n",
    "                    t = scale_time(row['Hour'])\n",
    "                    z = variable_encoder.transform([var])[0]  # Encode variable\n",
    "                    v = value_scalers[var].transform([[row[var]]])[0][0]  # Scale value\n",
    "                    tokenized_data.append([patient_id, t, z, v])\n",
    "    \n",
    "    return pd.DataFrame(tokenized_data, columns=['PatientID', 't', 'z', 'v'])\n",
    "\n",
    "# Process datasets\n",
    "for set_id in ['a', 'b', 'c']:\n",
    "    print(f\"Tokenizing set {set_id}...\")\n",
    "    df = pd.read_parquet(os.path.join(input_path, f\"set-{set_id}.parquet\"))\n",
    "    tokenized_df = tokenize_patient_data(df)\n",
    "    tokenized_df.to_parquet(os.path.join(output_path, f\"tokenized-set-{set_id}.parquet\"))\n",
    "    print(f\"Saved tokenized set {set_id} with {len(tokenized_df)} rows.\")\n",
    "\n",
    "print(\"Tokenization complete. Data saved to tokenized_data/\")\n",
    "\n",
    "# Load tokenized datasets\n",
    "def load_tokenized_data(set_id):\n",
    "    return pd.read_parquet(f\"tokenized_data/tokenized-set-{set_id}.parquet\")\n",
    "\n",
    "train_df = load_tokenized_data('a')\n",
    "test_df = load_tokenized_data('c')  # Test set\n",
    "\n",
    "# Prepare input tensors\n",
    "max_seq_len = 500  # Maximum sequence length\n",
    "num_variables = 41  # Number of unique variables (z values)\n",
    "\n",
    "# Pad sequences to max length\n",
    "def prepare_sequences(df, max_seq_len):\n",
    "    grouped = df.groupby(\"PatientID\").apply(lambda x: x.sort_values(\"t\").iloc[:max_seq_len])\n",
    "    X_t = grouped[\"t\"].groupby(\"PatientID\").apply(lambda x: np.pad(x.values, (0, max_seq_len - len(x)), 'constant'))\n",
    "    X_z = grouped[\"z\"].groupby(\"PatientID\").apply(lambda x: np.pad(x.values, (0, max_seq_len - len(x)), 'constant'))\n",
    "    X_v = grouped[\"v\"].groupby(\"PatientID\").apply(lambda x: np.pad(x.values, (0, max_seq_len - len(x)), 'constant'))\n",
    "    y = grouped[\"PatientID\"].apply(lambda x: x[\"v\"].iloc[0])  # Dummy target (modify if needed)\n",
    "    return np.stack([X_t, X_z, X_v], axis=-1), y.values\n",
    "\n",
    "X_train, y_train = prepare_sequences(train_df, max_seq_len)\n",
    "X_test, y_test = prepare_sequences(test_df, max_seq_len)\n",
    "\n",
    "# Transformer model definition\n",
    "def build_transformer_model(input_shape, num_variables, d_model=64, num_heads=4, ff_dim=128):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Variable Embedding\n",
    "    time_input = inputs[:, :, 0:1]\n",
    "    variable_input = Embedding(input_dim=num_variables, output_dim=d_model)(inputs[:, :, 1])\n",
    "    value_input = Dense(d_model)(inputs[:, :, 2:3])\n",
    "    \n",
    "    # Combine embeddings\n",
    "    x = tf.keras.layers.Concatenate()([time_input, variable_input, value_input])\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "    attn_output = Dropout(0.1)(attn_output)\n",
    "    attn_output = LayerNormalization(epsilon=1e-6)(attn_output + x)\n",
    "    \n",
    "    # Feed-Forward Layer\n",
    "    ff_output = Dense(ff_dim, activation='relu')(attn_output)\n",
    "    ff_output = Dense(d_model)(ff_output)\n",
    "    ff_output = Dropout(0.1)(ff_output)\n",
    "    ff_output = LayerNormalization(epsilon=1e-6)(ff_output + attn_output)\n",
    "    \n",
    "    # Classification Head\n",
    "    x = Flatten()(ff_output)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "# Build and train model\n",
    "model = build_transformer_model((max_seq_len, 3), num_variables)\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_probs = model.predict(X_test).flatten()\n",
    "auroc = roc_auc_score(y_test, y_pred_probs)\n",
    "auprc = average_precision_score(y_test, y_pred_probs)\n",
    "\n",
    "print(f\"Test AUROC: {auroc:.4f}\")\n",
    "print(f\"Test AUPRC: {auprc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

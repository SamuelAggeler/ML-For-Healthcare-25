{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1 Classic Machine Learning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tsfresh.feature_extraction import extract_features, MinimalFCParameters\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import make_forecasting_frame\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - AUROC: 0.8468, AUPRC: 0.5152\n",
      "Random Forest - AUROC: 0.8346, AUPRC: 0.5078\n",
      "SVM - AUROC: 0.8309, AUPRC: 0.4873\n"
     ]
    }
   ],
   "source": [
    "# === PART 1 OF Q2.1 ===\n",
    "# Load preprocessed datasets\n",
    "df_a = pd.read_parquet('ml_ready_data/set-a-scaled.parquet')\n",
    "df_b = pd.read_parquet('ml_ready_data/set-b-scaled.parquet')\n",
    "df_c = pd.read_parquet('ml_ready_data/set-c-scaled.parquet')  # Test set\n",
    "\n",
    "df_train = df_a\n",
    "df_validate = df_b\n",
    "df_test = df_c  # Test set is set-c\n",
    "\n",
    "# Define static and dynamic variables\n",
    "static_vars = ['Age', 'Gender', 'Height', 'Weight']\n",
    "time_series_vars = [col for col in df_train.columns if col not in static_vars + ['Hour', 'PatientID', 'RecordID', 'In_hospital_death']]\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(df):\n",
    "    features = df.groupby('PatientID')[time_series_vars].agg(['mean', 'max', 'last'])\n",
    "    features.columns = ['_'.join(col) for col in features.columns]\n",
    "    \n",
    "    # Add static variables\n",
    "    static_data = df.groupby('PatientID')[static_vars].last()\n",
    "    features = features.merge(static_data, left_index=True, right_index=True)\n",
    "    \n",
    "    # Add labels\n",
    "    labels = df.groupby('PatientID')['In_hospital_death'].last()\n",
    "    features['In_hospital_death'] = labels\n",
    "    return features.reset_index()\n",
    "\n",
    "# Extract features\n",
    "df_train_features = extract_features(df_train)\n",
    "df_test_features = extract_features(df_test)\n",
    "\n",
    "# Prepare data for ML\n",
    "X_train = df_train_features.drop(columns=['PatientID', 'In_hospital_death'])\n",
    "y_train = df_train_features['In_hospital_death']\n",
    "X_test = df_test_features.drop(columns=['PatientID', 'In_hospital_death'])\n",
    "y_test = df_test_features['In_hospital_death']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train classifiers\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(model, X_test, y_test, name):\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    auprc = average_precision_score(y_test, y_pred)\n",
    "    print(f\"{name} - AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "# Report performance\n",
    "evaluate_model(logreg, X_test, y_test, 'Logistic Regression')\n",
    "evaluate_model(rf, X_test, y_test, 'Random Forest')\n",
    "evaluate_model(svm_model, X_test, y_test, 'SVM')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(df_a.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ts_features.shape)\n",
    "df_long = df_train[['PatientID', 'Hour'] + time_series_vars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 164000/164000 [00:41<00:00, 3956.94it/s]\n",
      "Feature Extraction: 100%|██████████| 164000/164000 [00:40<00:00, 4021.38it/s]\n",
      "/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - AUROC: 0.8109, AUPRC: 0.4315\n",
      "Random Forest - AUROC: 0.8162, AUPRC: 0.4654\n",
      "SVM - AUROC: 0.7862, AUPRC: 0.3974\n"
     ]
    }
   ],
   "source": [
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# === PART 2 OF Q2.1 ===\n",
    "\n",
    "# Extract features with tsfresh\n",
    "df_train = df_a\n",
    "df_validate = df_b\n",
    "df_test = df_c  # Test set is set-c\n",
    "\n",
    "# Define static and dynamic variables\n",
    "static_vars = ['Age', 'Gender', 'Height', 'Weight']\n",
    "time_series_vars = [col for col in df_train.columns if col not in static_vars + ['Hour', 'PatientID', 'RecordID', 'In_hospital_death']]\n",
    "\n",
    "# Extract features with tsfresh\n",
    "def extract_tsfresh_features(df):\n",
    "    # Extract time-series features using tsfresh\n",
    "    ts_features = extract_features(\n",
    "        df[time_series_vars + static_vars + ['PatientID', 'Hour']],  # Include necessary columns\n",
    "        column_id='PatientID',  # ID column for each patient\n",
    "        column_sort='Hour',  # Time-related sorting column\n",
    "        default_fc_parameters=MinimalFCParameters(),  # Set of features to extract\n",
    "        n_jobs=1  # Adjust as needed for parallel processing\n",
    "    )\n",
    "    \n",
    "    # Add static variables (like Age, Gender, Height, Weight) to the features\n",
    "    static_data = df.groupby('PatientID')[static_vars].last()  # Get last static data for each patient\n",
    "    ts_features = ts_features.merge(static_data, left_index=True, right_index=True)\n",
    "    \n",
    "    # Add the target variable 'In_hospital_death'\n",
    "    labels = df.groupby('PatientID')['In_hospital_death'].last()  # Get the last In_hospital_death value for each patient\n",
    "    ts_features['In_hospital_death'] = labels\n",
    "    \n",
    "    return ts_features.reset_index()\n",
    "\n",
    "# Extract features for training and testing sets using tsfresh\n",
    "df_train_features = extract_tsfresh_features(df_a)\n",
    "df_test_features = extract_tsfresh_features(df_c)\n",
    "\n",
    "# Prepare data for ML\n",
    "X_train = df_train_features.drop(columns=['In_hospital_death'])\n",
    "y_train = df_train.groupby('PatientID')['In_hospital_death'].last()\n",
    "X_test = df_test_features.drop(columns=['In_hospital_death'])\n",
    "y_test = df_test.groupby('PatientID')['In_hospital_death'].last()\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train classifiers\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(model, X_test, y_test, name):\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    auprc = average_precision_score(y_test, y_pred)\n",
    "    print(f\"{name} - AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "# Report performance\n",
    "evaluate_model(logreg, X_test, y_test, 'Logistic Regression')\n",
    "evaluate_model(rf, X_test, y_test, 'Random Forest')\n",
    "evaluate_model(svm_model, X_test, y_test, 'SVM')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.2 Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.4644\n",
      "Epoch [2/20], Loss: 0.3617\n",
      "Epoch [3/20], Loss: 0.3375\n",
      "Epoch [4/20], Loss: 0.3165\n",
      "Epoch [5/20], Loss: 0.3092\n",
      "Epoch [6/20], Loss: 0.2887\n",
      "Epoch [7/20], Loss: 0.2658\n",
      "Epoch [8/20], Loss: 0.2290\n",
      "Epoch [9/20], Loss: 0.1945\n",
      "Epoch [10/20], Loss: 0.1713\n",
      "Epoch [11/20], Loss: 0.1578\n",
      "Epoch [12/20], Loss: 0.2450\n",
      "Epoch [13/20], Loss: 0.1078\n",
      "Epoch [14/20], Loss: 0.0806\n",
      "Epoch [15/20], Loss: 0.0309\n",
      "Epoch [16/20], Loss: 0.1565\n",
      "Epoch [17/20], Loss: 0.0549\n",
      "Epoch [18/20], Loss: 0.0117\n",
      "Epoch [19/20], Loss: 0.0072\n",
      "Epoch [20/20], Loss: 0.0092\n",
      "LSTM Model - Test Set C Performance: AUROC: 0.6841, AUPRC: 0.2522\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "# === Implementing LSTM ===\n",
    "\n",
    "# === Prepare Data for LSTM ===\n",
    "def prepare_lstm_data(df, time_series_vars, static_vars):\n",
    "    time_series_data = df.groupby('PatientID')[time_series_vars].apply(lambda x: x.values)\n",
    "    static_data = df.groupby('PatientID')[static_vars].last()\n",
    "    labels = df.groupby('PatientID')['In_hospital_death'].last()\n",
    "\n",
    "    # Pad sequences to ensure uniform length\n",
    "    max_timesteps = max(time_series_data.apply(len))\n",
    "    time_series_data = np.array([np.pad(x, ((0, max_timesteps - len(x)), (0, 0)), mode='constant') for x in time_series_data])\n",
    "\n",
    "    return time_series_data, static_data.values, labels.values\n",
    "\n",
    "# === Prepare Training and Test Data ===\n",
    "X_train_ts, X_train_static, y_train = prepare_lstm_data(df_a, time_series_vars, static_vars)\n",
    "X_test_ts, X_test_static, y_test = prepare_lstm_data(df_c, time_series_vars, static_vars)\n",
    "\n",
    "# === Standardize Features ===\n",
    "scaler_static = StandardScaler()\n",
    "X_train_static = scaler_static.fit_transform(X_train_static)\n",
    "X_test_static = scaler_static.transform(X_test_static)\n",
    "\n",
    "scaler_ts = StandardScaler()\n",
    "X_train_ts = np.array([scaler_ts.fit_transform(x) for x in X_train_ts])\n",
    "X_test_ts = np.array([scaler_ts.transform(x) for x in X_test_ts])\n",
    "\n",
    "# === Convert to PyTorch Tensors ===\n",
    "X_train_ts_tensor = torch.tensor(X_train_ts, dtype=torch.float32)\n",
    "X_test_ts_tensor = torch.tensor(X_test_ts, dtype=torch.float32)\n",
    "X_train_static_tensor = torch.tensor(X_train_static, dtype=torch.float32)\n",
    "X_test_static_tensor = torch.tensor(X_test_static, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# === Define the LSTM Model ===\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_shape_ts, input_shape_static):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_shape_ts[1], hidden_size=64, batch_first=True, dropout=0.3)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=32, batch_first=True, dropout=0.3)\n",
    "        self.fc1 = nn.Linear(32 + input_shape_static, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_ts, x_static):\n",
    "        lstm_out, (hn, cn) = self.lstm1(x_ts)\n",
    "        lstm_out, (hn, cn) = self.lstm2(lstm_out)\n",
    "        last_lstm_output = lstm_out[:, -1, :]  # Use the last time-step output\n",
    "\n",
    "        # Concatenate static data with LSTM output\n",
    "        combined_input = torch.cat((last_lstm_output, x_static), dim=1)\n",
    "        x = torch.relu(self.fc1(combined_input))\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# === Initialize and Train the Model ===\n",
    "input_shape_ts = (X_train_ts.shape[1], X_train_ts.shape[2])  # (timesteps, features)\n",
    "input_shape_static = X_train_static.shape[1]  # Number of static features\n",
    "\n",
    "model = LSTMModel(input_shape_ts, input_shape_static)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# === Training Loop ===\n",
    "def train_model(model, X_train_ts, X_train_static, y_train, epochs=20, batch_size=32):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        # Create batches\n",
    "        for i in range(0, len(X_train_ts), batch_size):\n",
    "            batch_ts = X_train_ts[i:i+batch_size]\n",
    "            batch_static = X_train_static[i:i+batch_size]\n",
    "            batch_labels = y_train[i:i+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_ts, batch_static)\n",
    "            loss = criterion(outputs.squeeze(), batch_labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Print training progress\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# === Train the Model ===\n",
    "train_model(model, X_train_ts_tensor, X_train_static_tensor, y_train_tensor)\n",
    "\n",
    "# === Evaluate the Model ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_ts_tensor, X_test_static_tensor).squeeze()\n",
    "    y_pred = y_pred.numpy()\n",
    "    \n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    auprc = average_precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"LSTM Model - Test Set C Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.4288\n",
      "Epoch 2/20, Loss: 0.3454\n",
      "Epoch 3/20, Loss: 0.3211\n",
      "Epoch 4/20, Loss: 0.2941\n",
      "Epoch 5/20, Loss: 0.2726\n",
      "Epoch 6/20, Loss: 0.2497\n",
      "Epoch 7/20, Loss: 0.2283\n",
      "Epoch 8/20, Loss: 0.2054\n",
      "Epoch 9/20, Loss: 0.1916\n",
      "Epoch 10/20, Loss: 0.1792\n",
      "Epoch 11/20, Loss: 0.1666\n",
      "Epoch 12/20, Loss: 0.1513\n",
      "Epoch 13/20, Loss: 0.1379\n",
      "Epoch 14/20, Loss: 0.1264\n",
      "Epoch 15/20, Loss: 0.1136\n",
      "Epoch 16/20, Loss: 0.1125\n",
      "Epoch 17/20, Loss: 0.1070\n",
      "Epoch 18/20, Loss: 0.0844\n",
      "Epoch 19/20, Loss: 0.0955\n",
      "Epoch 20/20, Loss: 0.0886\n",
      "Bidirectional LSTM Model - Test Set C Performance: AUROC: 0.7373, AUPRC: 0.2933\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# === Define the Bidirectional LSTM Model in PyTorch ===\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_dim_ts, hidden_dim=64, dropout_rate=0.3):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_dim_ts, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim*2, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)  # Binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass through first bidirectional LSTM\n",
    "        x, _ = self.lstm1(x)  \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through second bidirectional LSTM\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Take the output from the last timestep\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return torch.sigmoid(x)  # Sigmoid for binary classification\n",
    "\n",
    "# === Data Preparation for PyTorch ===\n",
    "def prepare_lstm_data_for_pytorch(df, time_series_vars, static_vars):\n",
    "    time_series_data, static_data, labels = prepare_lstm_data(df, time_series_vars, static_vars)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    time_series_data = torch.tensor(time_series_data, dtype=torch.float32)\n",
    "    static_data = torch.tensor(static_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    return time_series_data, static_data, labels\n",
    "\n",
    "# Prepare training and test data\n",
    "X_train_ts, X_train_static, y_train = prepare_lstm_data_for_pytorch(df_a, time_series_vars, static_vars)\n",
    "X_test_ts, X_test_static, y_test = prepare_lstm_data_for_pytorch(df_c, time_series_vars, static_vars)\n",
    "\n",
    "# Create datasets and dataloaders for training and testing\n",
    "train_dataset = TensorDataset(X_train_ts, X_train_static, y_train)\n",
    "test_dataset = TensorDataset(X_test_ts, X_test_static, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# === Define the Training Function ===\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            time_series_data, _, labels = batch\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            outputs = model(time_series_data)  # Forward pass\n",
    "            \n",
    "            loss = criterion(outputs.squeeze(), labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# === Train the Model ===\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim_ts = X_train_ts.shape[2]  # Number of time series features\n",
    "model = BidirectionalLSTM(input_dim_ts)\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=20)\n",
    "\n",
    "# === Evaluate the Model ===\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            time_series_data, _, labels = batch\n",
    "            outputs = model(time_series_data)\n",
    "            \n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(outputs.squeeze().numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    auroc = roc_auc_score(y_true, y_pred)\n",
    "    auprc = average_precision_score(y_true, y_pred)\n",
    "    \n",
    "    return auroc, auprc\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "auroc, auprc = evaluate_model(model, test_loader)\n",
    "print(f\"Bidirectional LSTM Model - Test Set C Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.3a Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.4424\n",
      "Epoch 2/20, Loss: 0.3769\n",
      "Epoch 3/20, Loss: 0.3659\n",
      "Epoch 4/20, Loss: 0.3565\n",
      "Epoch 5/20, Loss: 0.3453\n",
      "Epoch 6/20, Loss: 0.3391\n",
      "Epoch 7/20, Loss: 0.3323\n",
      "Epoch 8/20, Loss: 0.3266\n",
      "Epoch 9/20, Loss: 0.3173\n",
      "Epoch 10/20, Loss: 0.3138\n",
      "Epoch 11/20, Loss: 0.3027\n",
      "Epoch 12/20, Loss: 0.2955\n",
      "Epoch 13/20, Loss: 0.2879\n",
      "Epoch 14/20, Loss: 0.2743\n",
      "Epoch 15/20, Loss: 0.2648\n",
      "Epoch 16/20, Loss: 0.2539\n",
      "Epoch 17/20, Loss: 0.2455\n",
      "Epoch 18/20, Loss: 0.2393\n",
      "Epoch 19/20, Loss: 0.2235\n",
      "Epoch 20/20, Loss: 0.2264\n",
      "Transformer Model - Test Set Performance: AUROC: 0.3562, AUPRC: 0.1118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, time_series_input_size, static_input_size, num_heads=4, hidden_dim=64, projected_dim=40):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Project time-series features to an embedding size divisible by num_heads\n",
    "        self.input_projection = nn.Linear(time_series_input_size, projected_dim)\n",
    "        \n",
    "        # Multi-Head Attention Layer\n",
    "        self.attention_layer = nn.MultiheadAttention(embed_dim=projected_dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # Fully connected layers after attention\n",
    "        self.fc1 = nn.Linear(projected_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Process static features\n",
    "        self.static_fc = nn.Linear(static_input_size, hidden_dim)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, time_series_input, static_input):\n",
    "        # Project time-series features\n",
    "        time_series_input = self.input_projection(time_series_input)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_output, _ = self.attention_layer(time_series_input, time_series_input, time_series_input)\n",
    "        \n",
    "        # Pooling\n",
    "        x = attention_output.mean(dim=1)  # Global average pooling\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        # Process static features\n",
    "        static_out = self.static_fc(static_input)\n",
    "        \n",
    "        # Combine time-series and static features\n",
    "        combined = x + static_out  # Element-wise addition\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        # Final output\n",
    "        output = self.output_fc(combined)\n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "# Load datasets\n",
    "df_a = pd.read_parquet('ml_ready_data/set-a-scaled.parquet')\n",
    "df_b = pd.read_parquet('ml_ready_data/set-b-scaled.parquet')\n",
    "df_c = pd.read_parquet('ml_ready_data/set-c-scaled.parquet')\n",
    "\n",
    "df_train = df_a\n",
    "df_validate = df_b\n",
    "df_test = df_c\n",
    "\n",
    "# Define static and time-series variables\n",
    "static_vars = ['Age', 'Gender', 'Height', 'Weight']\n",
    "time_series_vars = [col for col in df_train.columns if col not in static_vars + ['Hour', 'PatientID', 'RecordID', 'In_hospital_death']]\n",
    "\n",
    "def prepare_data(df):\n",
    "    grouped = df.groupby('PatientID')\n",
    "    \n",
    "    # Time-series tensor: (num_patients, seq_len, num_features)\n",
    "    time_series_data = np.array([group[time_series_vars].values for _, group in grouped])\n",
    "    static_data = grouped[static_vars].last().values  # Static features (num_patients, num_static_features)\n",
    "    labels = grouped['In_hospital_death'].last().values  # Labels\n",
    "    \n",
    "    return time_series_data, static_data, labels\n",
    "\n",
    "# Prepare training & test sets\n",
    "X_train_ts, X_train_static, y_train = prepare_data(df_train)\n",
    "X_test_ts, X_test_static, y_test = prepare_data(df_test)\n",
    "\n",
    "# Standardization\n",
    "scaler_ts = StandardScaler()\n",
    "scaler_static = StandardScaler()\n",
    "\n",
    "X_train_ts = np.array([scaler_ts.fit_transform(seq) for seq in X_train_ts])\n",
    "X_test_ts = np.array([scaler_ts.transform(seq) for seq in X_test_ts])\n",
    "\n",
    "X_train_static = scaler_static.fit_transform(X_train_static)\n",
    "X_test_static = scaler_static.transform(X_test_static)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_ts = torch.tensor(X_train_ts, dtype=torch.float32)\n",
    "X_train_static = torch.tensor(X_train_static, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_ts = torch.tensor(X_test_ts, dtype=torch.float32)\n",
    "X_test_static = torch.tensor(X_test_static, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_ts, X_train_static, y_train)\n",
    "test_dataset = TensorDataset(X_test_ts, X_test_static, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Model Initialization\n",
    "model = TransformerModel(time_series_input_size=len(time_series_vars), static_input_size=len(static_vars))\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_ts, batch_static, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_ts, batch_static)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_preds = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for batch_ts, batch_static, batch_y in test_loader:\n",
    "        output = model(batch_ts, batch_static).cpu().numpy()\n",
    "        y_preds.extend(output)\n",
    "        y_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "y_preds = np.array(y_preds).flatten()\n",
    "y_true = np.array(y_true).flatten()\n",
    "\n",
    "auroc = roc_auc_score(y_true, y_preds)\n",
    "auprc = average_precision_score(y_true, y_preds)\n",
    "print(f\"Transformer Model - Test Set Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.3b Tokenizing Time-Series Data and Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m patient_groups_validate = df_validate.groupby(\u001b[33m'\u001b[39m\u001b[33mPatientID\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     56\u001b[39m patient_groups_test = df_test.groupby(\u001b[33m'\u001b[39m\u001b[33mPatientID\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m all_patients_train = [\u001b[43mprocess_patient_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalers\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _, group \u001b[38;5;129;01min\u001b[39;00m patient_groups_train]\n\u001b[32m     59\u001b[39m all_patients_validate = [process_patient_data(group, scalers) \u001b[38;5;28;01mfor\u001b[39;00m _, group \u001b[38;5;129;01min\u001b[39;00m patient_groups_validate]\n\u001b[32m     60\u001b[39m all_patients_test = [process_patient_data(group, scalers) \u001b[38;5;28;01mfor\u001b[39;00m _, group \u001b[38;5;129;01min\u001b[39;00m patient_groups_test]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mprocess_patient_data\u001b[39m\u001b[34m(patient_df, scalers, max_seq_len)\u001b[39m\n\u001b[32m     26\u001b[39m t = patient_df[\u001b[33m'\u001b[39m\u001b[33mt\u001b[39m\u001b[33m'\u001b[39m].values\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Scale values (v) for all time_series_vars at once\u001b[39;00m\n\u001b[32m     29\u001b[39m v_scaled = np.column_stack([\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[43mscalers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatient_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m time_series_vars\n\u001b[32m     31\u001b[39m ])\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[32m1\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Encode variable types (z) for all time_series_vars\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1062\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1059\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1061\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1074\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/sklearn/utils/validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/sklearn/utils/validation.py:991\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    988\u001b[39m context = \u001b[33m\"\u001b[39m\u001b[33m by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % estimator_name \u001b[38;5;28;01mif\u001b[39;00m estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    990\u001b[39m \u001b[38;5;66;03m# When all dataframe columns are sparse, convert to a sparse array\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msparse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m array.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m    992\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mImportError\u001b[39;00m):\n\u001b[32m    993\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/pandas/core/accessor.py:224\u001b[39m, in \u001b[36mCachedAccessor.__get__\u001b[39m\u001b[34m(self, obj, cls)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessor\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m accessor_obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m._name, accessor_obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/pandas/core/arrays/sparse/accessor.py:31\u001b[39m, in \u001b[36mBaseAccessor.__init__\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data=\u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mself\u001b[39m._parent = data\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/pandas/core/arrays/sparse/accessor.py:247\u001b[39m, in \u001b[36mSparseFrameAccessor._validate\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     dtypes = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtypes\u001b[49m\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(t, SparseDtype) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m dtypes):\n\u001b[32m    249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m._validation_msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/pandas/core/generic.py:6461\u001b[39m, in \u001b[36mNDFrame.dtypes\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   6434\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6435\u001b[39m \u001b[33;03mReturn the dtypes in the DataFrame.\u001b[39;00m\n\u001b[32m   6436\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   6458\u001b[39m \u001b[33;03mdtype: object\u001b[39;00m\n\u001b[32m   6459\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6460\u001b[39m data = \u001b[38;5;28mself\u001b[39m._mgr.get_dtypes()\n\u001b[32m-> \u001b[39m\u001b[32m6461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_constructor_sliced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobject_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/pandas/core/series.py:593\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    590\u001b[39m         data = SingleArrayManager.from_array(data, index)\n\u001b[32m    592\u001b[39m NDFrame.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m = name\n\u001b[32m    594\u001b[39m \u001b[38;5;28mself\u001b[39m._set_axis(\u001b[32m0\u001b[39m, index)\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m original_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_pandas_object \u001b[38;5;129;01mand\u001b[39;00m data_dtype == np.object_:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/pandas/core/generic.py:6320\u001b[39m, in \u001b[36mNDFrame.__setattr__\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   6317\u001b[39m \u001b[38;5;66;03m# if this fails, go on to more involved attribute setting\u001b[39;00m\n\u001b[32m   6318\u001b[39m \u001b[38;5;66;03m# (note that this matches __getattr__, above).\u001b[39;00m\n\u001b[32m   6319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._internal_names_set:\n\u001b[32m-> \u001b[39m\u001b[32m6320\u001b[39m     \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6321\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._metadata:\n\u001b[32m   6322\u001b[39m     \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/jupyter/lib/python3.12/site-packages/pandas/core/series.py:784\u001b[39m, in \u001b[36mSeries.name\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    737\u001b[39m \u001b[33;03m    Return the name of the Series.\u001b[39;00m\n\u001b[32m    738\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    780\u001b[39m \u001b[33;03m    'Even Numbers'\u001b[39;00m\n\u001b[32m    781\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    782\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._name\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m \u001b[38;5;129m@name\u001b[39m.setter\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mname\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: Hashable) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    786\u001b[39m     validate_all_hashable(value, error_name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.name\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    787\u001b[39m     \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_name\u001b[39m\u001b[33m\"\u001b[39m, value)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df_a', 'df_b', 'df_c' are already loaded\n",
    "df_train = df_a\n",
    "df_validate = df_b\n",
    "df_test = df_c\n",
    "\n",
    "# Define static and time-series variables\n",
    "static_vars = ['Age', 'Gender', 'Height', 'Weight']\n",
    "time_series_vars = [col for col in df_train.columns if col not in static_vars + ['Hour', 'PatientID', 'RecordID', 'In_hospital_death']]\n",
    "\n",
    "# Normalize time for each dataset\n",
    "df_train['t'] = df_train['Hour'] / df_train['Hour'].max()\n",
    "df_validate['t'] = df_validate['Hour'] / df_train['Hour'].max()\n",
    "df_test['t'] = df_test['Hour'] / df_train['Hour'].max()\n",
    "\n",
    "# One-hot encode variable types (z)\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "z_encoded = enc.fit_transform(np.array(time_series_vars).reshape(-1, 1))\n",
    "var_dict = {var: z_encoded[i] for i, var in enumerate(time_series_vars)}\n",
    "\n",
    "def process_patient_data(patient_df, scalers, max_seq_len=500):\n",
    "    # Normalize time (t)\n",
    "    t = patient_df['t'].values\n",
    "    \n",
    "    # Scale values (v) for all time_series_vars at once\n",
    "    v_scaled = np.column_stack([\n",
    "        scalers[var].transform(patient_df[[var]].fillna(0))[:, 0] for var in time_series_vars\n",
    "    ])\n",
    "\n",
    "    # Encode variable types (z) for all time_series_vars\n",
    "    z_encoded = np.array([var_dict[var] for var in time_series_vars])\n",
    "\n",
    "    # Repeat t for each variable and flatten\n",
    "    t_repeated = np.repeat(t, len(time_series_vars))\n",
    "\n",
    "    # Tile z_encoded for each time step\n",
    "    z_tiled = np.tile(z_encoded, (len(t), 1))\n",
    "\n",
    "    # Flatten v_scaled\n",
    "    v_flattened = v_scaled.flatten()\n",
    "\n",
    "    # Combine into triplets (t, z, v)\n",
    "    triplets = [(t_repeated[i], z_tiled[i], v_flattened[i]) for i in range(len(t_repeated))]\n",
    "\n",
    "    return triplets\n",
    "\n",
    "# Fit scalers on training set\n",
    "scalers = {var: StandardScaler().fit(df_train[[var]].dropna()) for var in time_series_vars}\n",
    "\n",
    "# Process datasets\n",
    "patient_groups_train = df_train.groupby('PatientID')\n",
    "patient_groups_validate = df_validate.groupby('PatientID')\n",
    "patient_groups_test = df_test.groupby('PatientID')\n",
    "\n",
    "all_patients_train = [process_patient_data(group, scalers) for _, group in patient_groups_train]\n",
    "all_patients_validate = [process_patient_data(group, scalers) for _, group in patient_groups_validate]\n",
    "all_patients_test = [process_patient_data(group, scalers) for _, group in patient_groups_test]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_ts = [torch.tensor([x[0] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_train]\n",
    "X_train_z = [torch.tensor([x[1] for x in seq], dtype=torch.float32) for seq in all_patients_train]\n",
    "X_train_v = [torch.tensor([x[2] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_train]\n",
    "\n",
    "X_validate_ts = [torch.tensor([x[0] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_validate]\n",
    "X_validate_z = [torch.tensor([x[1] for x in seq], dtype=torch.float32) for seq in all_patients_validate]\n",
    "X_validate_v = [torch.tensor([x[2] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_validate]\n",
    "\n",
    "X_test_ts = [torch.tensor([x[0] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_test]\n",
    "X_test_z = [torch.tensor([x[1] for x in seq], dtype=torch.float32) for seq in all_patients_test]\n",
    "X_test_v = [torch.tensor([x[2] for x in seq], dtype=torch.float32).unsqueeze(1) for seq in all_patients_test]\n",
    "\n",
    "# Dataset preparation (combining t, z, v into a single tensor)\n",
    "train_data = [(torch.cat((t, z, v), dim=1), torch.tensor(0.0)) for t, z, v in zip(X_train_ts, X_train_z, X_train_v)]\n",
    "validate_data = [(torch.cat((t, z, v), dim=1), torch.tensor(0.0)) for t, z, v in zip(X_validate_ts, X_validate_z, X_validate_v)]\n",
    "test_data = [(torch.cat((t, z, v), dim=1), torch.tensor(0.0)) for t, z, v in zip(X_test_ts, X_test_z, X_test_v)]\n",
    "\n",
    "# Convert to DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "validate_loader = DataLoader(validate_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, time_series_input_size, num_heads=4, hidden_dim=64, projected_dim=40):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Project time-series features to an embedding size divisible by num_heads\n",
    "        self.input_projection = nn.Linear(time_series_input_size, projected_dim)\n",
    "        \n",
    "        # Multi-Head Attention Layer\n",
    "        self.attention_layer = nn.MultiheadAttention(embed_dim=projected_dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # Fully connected layers after attention\n",
    "        self.fc1 = nn.Linear(projected_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, time_series_input):\n",
    "        # Apply attention\n",
    "        attention_output, _ = self.attention_layer(time_series_input, time_series_input, time_series_input)\n",
    "        \n",
    "        # Pooling\n",
    "        x = attention_output.mean(dim=1)  # Global average pooling\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        # Final output\n",
    "        output = self.output_fc(x)\n",
    "        return torch.sigmoid(output)\n",
    "# Training Loop\n",
    "epochs = 20\n",
    "model = TransformerModel(time_series_input_size=1 + 41 + 1)  # t, z, v\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_preds = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        output = model(batch_x).cpu().numpy()\n",
    "        y_preds.extend(output)\n",
    "        y_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "y_preds = np.array(y_preds).flatten()\n",
    "y_true = np.array(y_true).flatten()\n",
    "\n",
    "auroc = roc_auc_score(y_true, y_preds)\n",
    "auprc = average_precision_score(y_true, y_preds)\n",
    "print(f\"Transformer Model - Test Set Performance: AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
